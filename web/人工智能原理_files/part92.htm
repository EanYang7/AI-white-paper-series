<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-cn" lang="zh-cn"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>5.3.3 优化能力</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part91.htm">&lt; 上一个</a><span> | </span><a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8E%9F%E7%90%86.html">内容</a><span> | </span><a href="part93.htm">下一个 &gt;</a></p><p class="s14" style="padding-left: 7pt;text-indent: 0pt;line-height: 21pt;text-align: left;"><a name="bookmark94">5.3.3 </a><span class="h4">优化能力</span></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">神经网络的优化研究通常基于非凸优化理论。这方面的主流研究工作试图对优化器收敛到最优点的迭代复杂度进行刻画，并基于此对不同的优化器进行比较。其中代表结果是随机梯度下降在非凸目标函数上可以收敛到驻点。</p><p style="padding-left: 34pt;text-indent: 0pt;text-align: left;">然而，这些优化分析与神经网络的实际应用仍有较大差距。首先，</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: justify;">自适应算法是训练神经网络的主流优化器，但是大多数自适应算法在非凸目标函数上的收敛性还没有得到刻画；其次，经典收敛性分析往往假设目标函数曲率有限，然而神经网络曲率变化剧烈。</p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part91.htm">&lt; 上一个</a><span> | </span><a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8E%9F%E7%90%86.html">内容</a><span> | </span><a href="part93.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
