<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-cn" lang="zh-cn"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>3 神经网络的计算原理：第 6 章参考文献</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part328.htm">&lt; 上一个</a><span> | </span><a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8E%9F%E7%90%86.html">内容</a><span> | </span><a href="part330.htm">下一个 &gt;</a></p><p class="s8" style="padding-top: 6pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a name="bookmark536">3 </a><span class="s9">神经网络的计算原理：第 </span>6 <span class="s9">章参考文献</span></p><p class="s137" style="padding-top: 7pt;padding-left: 43pt;text-indent: -26pt;line-height: 155%;text-align: justify;"><a name="bookmark537">[1] </a><span class="s10">Fukushima K. Neocognitron: A Self-Organizing Neural Network Model For A Mechanism Of Pattern Recognition Unaffected By Shift In Position[J]. Biological Cybernetics, 1980, 36(4): 193-202.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -26pt;line-height: 155%;text-align: justify;">[2] <span class="s10">Lecun Y, Boser B, Denker J S, Et Al. Backpropagation Applied To Handwritten Zip Code Recognition[J]. Neural Computation, 1989, 1(4): 541-551.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -26pt;line-height: 155%;text-align: justify;"><a name="bookmark538">[3] </a><span class="s10">Krizhevsky A, Sutskever I, Hinton G E. Imagenet Classification With Deep Convolutional Neural Networks[J]. Advances In Neural Information Processing Systems, 2012, 25.</span></p><p class="s137" style="padding-left: 17pt;text-indent: 0pt;text-align: justify;"><a name="bookmark539">[4] </a><span class="s10">Rumelhart D E, Hinton G E, Williams R J. Learning Representations</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 4pt;padding-left: 43pt;text-indent: 0pt;text-align: justify;">By Back-Propagating Errors[J]. Nature, 1986, 323(6088): 533-536.</p><p class="s137" style="padding-top: 8pt;padding-left: 43pt;text-indent: -26pt;line-height: 156%;text-align: justify;"><a name="bookmark540">[5] </a><span class="s10">Kingma D P, Welling M. Auto-Encoding Variational Bayes[J]. Arxiv Preprint Arxiv:1312.6114, 2013.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -26pt;line-height: 155%;text-align: justify;"><a name="bookmark541">[6] </a><span class="s10">Goodfellow I, Pouget-Abadie J, Mirza M, Et Al. Generative Adversarial Nets[J]. Advances In Neural Information Processing Systems, 2014, 27.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -26pt;line-height: 156%;text-align: justify;"><a name="bookmark542">[7] </a><span class="s10">Vaswani A, Shazeer N, Parmar N, Et Al. Attention Is All You Need[J]. Advances In Neural Information Processing Systems, 2017, 30.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -26pt;line-height: 155%;text-align: justify;"><a name="bookmark543">[8] </a><span class="s10">Devlin J, Chang M W, Lee K, Et Al. Bert: Pre-Training Of Deep Bidirectional Transformers For Language Understanding[J]. Arxiv Preprint Arxiv:1810.04805, 2018.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -26pt;line-height: 155%;text-align: justify;"><a name="bookmark544">[9] </a><span class="s10">Radford A, Narasimhan K, Salimans T, Et Al. Improving Language Understanding By Generative Pre-Training[J]. 2018.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark545">[10] </a><span class="s10">Mikolov T, Chen K, Corrado G, Et Al. Efficient Estimation Of Word Representations In Vector Space[J]. Arxiv Preprint Arxiv:1301.3781, 2013.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark546">[11] </a><span class="s10">Pennington J, Socher R, Manning C D. Glove: Global Vectors For Word Representation[C]//Proceedings Of The 2014 Conference On Empirical Methods In Natural Language Processing (EMNLP). 2014: 1532-1543.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark547">[12] </a><span class="s10">Chowdhury G G. Introduction To Modern Information Retrieval[M]. Facet Publishing, 2010.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark548">[13] </a><span class="s10">Liu Y, Ott M, Goyal N, Et Al. Roberta: A Robustly Optimized Bert Pretraining Approach[J]. Arxiv Preprint Arxiv:1907.11692, 2019.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 156%;text-align: justify;"><a name="bookmark549">[14] </a><span class="s10">Rosenblatt, F. (1958). The Perceptron: A Probabilistic Model For Information Storage And Organization In The Brain. Psychological</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 4pt;padding-left: 43pt;text-indent: 0pt;text-align: justify;">Review, 65(6), 386.</p><p class="s137" style="padding-top: 8pt;padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark550">[15] </a><span class="s10">Hastie, T., Tibshirani, R., Friedman, J. H., &amp; Friedman, J. H. (2009). The Elements Of Statistical Learning: Data Mining, Inference, And Prediction (Vol. 2, Pp. 1-758). New York: Springer.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark551">[16] </a><span class="s10">Lecun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., &amp; Jackel, L. D. (1989). Backpropagation Applied To Handwritten Zip Code Recognition. Neural Computation, 1(4), 541- 551.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[17] <span class="s10">Lecun, Y. (1989). Generalization And Network Design Strategies. Connectionism In Perspective, 19(143-155), 18.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[18] <span class="s10">Lecun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W., &amp; Jackel, L. (1989). Handwritten Digit Recognition With A Back-Propagation Network. Advances In Neural Information Processing Systems, 2.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark552">[19] </a><span class="s10">Lecun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient- Based Learning Applied To Document Recognition. Proceedings Of The IEEE, 86(11), 2278-2324.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark553">[20] </a><span class="s10">Ioffe, S., &amp; Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training By Reducing Internal Covariate Shift. Arxiv Preprint Arxiv:1502.03167.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark554">[21] </a><span class="s10">Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014). Dropout: A Simple Way To Prevent Neural Networks From Overfitting. The Journal Of Machine Learning Research, 15(1), 1929-1958.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark555">[22] </a><span class="s10">Kingma, D. P., &amp; Ba, J. (2014). Adam: A Method For Stochastic Optimization. Arxiv Preprint Arxiv:1412.6980.</span></p><p class="s137" style="padding-left: 13pt;text-indent: 0pt;text-align: justify;">[23] <span class="s10">Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 4pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">Method. Arxiv Preprint Arxiv:1212.5701.</p><p class="s137" style="padding-top: 8pt;padding-left: 43pt;text-indent: -30pt;line-height: 156%;text-align: justify;">[24] <span class="s10">Reddi, S. J., Kale, S., &amp; Kumar, S. (2018). On The Convergence Of Adam And Beyond. ICLR.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark556">[25] </a><span class="s10">Jordan, M. I. (1997). Serial Order: A Parallel Distributed Processing Approach. In Advances In Psychology (Vol. 121, Pp. 471-495). North-Holland.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[26] <span class="s10">Elman, J. L. (1990). Finding Structure In Time. Cognitive Science, 14(2), 179-211.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark557">[27] </a><span class="s10">Hochreiter, S., &amp; Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation, 9(8), 1735-1780.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark558">[28] </a><span class="s10">Cho, K., Van Merriënboer, B., Bahdanau, D., &amp; Bengio, Y. (2014). On The Properties Of Neural Machine Translation: Encoder- Decoder Approaches. Arxiv Preprint Arxiv:1409.1259.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark559">[29] </a><span class="s10">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). Attention Is All You Need. Advances In Neural Information Processing Systems, 30.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark560">[30] </a><span class="s10">Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-Training Of Deep Bidirectional Transformers For Language Understanding. Arxiv Preprint Arxiv:1810.04805.</span></p><p class="s137" style="padding-left: 13pt;text-indent: 0pt;text-align: justify;"><a name="bookmark561">[31] </a><span class="s10">Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever,</span></p><p class="s10" style="padding-top: 9pt;padding-left: 43pt;text-indent: 0pt;line-height: 155%;text-align: justify;">I. (2019). Language Models Are Unsupervised Multitask Learners. Openai Blog, 1(8), 9.</p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[32] <span class="s10">Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., &amp; Stoyanov, V. (2019). Roberta: A Robustly Optimized BERT Pretraining Approach. Arxiv Preprint Arxiv:1907.11692.</span></p><p class="s137" style="padding-left: 13pt;text-indent: 0pt;text-align: justify;"><a name="bookmark562">[33] </a><span class="s10">Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M.,</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 4pt;padding-left: 43pt;text-indent: 0pt;line-height: 155%;text-align: justify;">Zhou, Y., Li, W., &amp; Liu, P. J. (2020). Exploring The Limits Of Transfer Learning With A Unified Text-To-Text Transformer. Journal Of Machine Learning Research, 21(140), 1-67.</p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark563">[34] </a><span class="s10">Yan, S., Xiong, Y., &amp; Lin, D. (2021). Spatial Temporal Graph Convolutional Networks For Skeleton-Based Action Recognition. Proceedings Of The AAAI Conference On Artificial Intelligence, 35(1), 607-615.</span></p><p class="s137" style="padding-left: 13pt;text-indent: 0pt;text-align: justify;">[35] <span class="s10">Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., &amp; Salakhutdinov,</span></p><p class="s10" style="padding-top: 9pt;padding-left: 43pt;text-indent: 0pt;line-height: 155%;text-align: justify;">R. (2019). Transformer-Xl: Attentive Language Models Beyond A Fixed-Length Context. Arxiv Preprint Arxiv:1901.02860.</p><p class="s137" style="padding-left: 13pt;text-indent: 0pt;text-align: justify;">[36] <span class="s10">Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., &amp; Soricut,</span></p><p class="s10" style="padding-top: 9pt;padding-left: 43pt;text-indent: 0pt;line-height: 155%;text-align: justify;">R. (2020). ALBERT: A Lite BERT For Self-Supervised Learning Of Language Representations. ICLR.</p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark564">[37] </a><span class="s10">Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ... &amp; Zettlemoyer, L. (2020). BART: Denoising Sequence- To-Sequence Pre-Training For Natural Language Generation, Translation, And Comprehension. Arxiv Preprint Arxiv:1910.13461.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark565">[38] </a><span class="s10">Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... &amp; Amodei, D. (2020). Language Models Are Few- Shot Learners. Arxiv Preprint Arxiv:2005.14165.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark566">[39] </a><span class="s10">Kang, D., Khot, T., Sabharwal, A., &amp; Hovy, E. (2020). Data Augmentation For Deep Learning: A Survey. Arxiv Preprint Arxiv:2108.10329.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark567">[40] </a><span class="s10">Yan, Y., Bi, W., Li, X., Wu, W., Zhang, Z., &amp; Tu, Z. (2021). VATT: Transformers For Multimodal Self-Supervised Learning From Raw Video, Audio And Text. Neurips.</span></p><p class="s137" style="padding-left: 13pt;text-indent: 0pt;text-align: justify;"><a name="bookmark568">[41] </a><span class="s10">Graves, A., Mohamed, A. R., &amp; Hinton, G. (2013). Speech</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="558" height="1" alt="image" src="Image_128.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 4pt;padding-left: 43pt;text-indent: 0pt;line-height: 155%;text-align: justify;">Recognition With Deep Recurrent Neural Networks. IEEE International Conference On Acoustics, Speech And Signal Processing.</p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[42] <span class="s10">Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). Sequence To Sequence Learning With Neural Networks. NIPS.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[43] <span class="s10">Luong, M. T., Pham, H., &amp; Manning, C. D. (2015). Effective Approaches To Attention-Based Neural Machine Translation. EMNLP.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[44] <span class="s10">Britz, D., Goldie, A., Luong, M. T., &amp; Le, Q. V. (2017). Massive Exploration Of Neural Machine Translation Architectures. EMNLP.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[45] <span class="s10">Edunov, S., Ott, M., Auli, M., Grangier, D., &amp; Ranzato, M. (2018). Classical Structured Prediction Losses For Sequence To Sequence Learning. NAACL.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[46] <span class="s10">Gu, J., Wang, Y., Chen, Y., Li, V. O., &amp; Cho, K. (2019). Meta- Learning For Low-Resource Neural Machine Translation. EMNLP- IJCNLP.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[47] <span class="s10">Bai, S., Kolter, J. Z., &amp; Koltun, V. (2018). An Empirical Evaluation Of Generic Convolutional And Recurrent Networks For Sequence Modeling. Arxiv Preprint Arxiv:1803.01271.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 156%;text-align: justify;">[48] <span class="s10">Gehring, J., Auli, M., Grangier, D., Yarats, D., &amp; Dauphin, Y. N. (2017). Convolutional Sequence To Sequence Learning. ICML.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[49] <span class="s10">Bello, I., Zoph, B., Vaswani, A., Shlens, J., &amp; Le, Q. V. (2019). Attention Augmented Convolutional Networks. ICCV.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark569">[50] </a><span class="s10">Perozzi, B., Al-Rfou, R., &amp; Skiena, S. (2014). Deepwalk: Online Learning Of Social Representations. Proceedings Of The 20th ACM SIGKDD International Conference On Knowledge Discovery And Data Mining (Pp. 701-710).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s137" style="padding-top: 4pt;padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark570">[51] </a><span class="s10">Grover, A., &amp; Leskovec, J. (2016). Node2vec: Scalable Feature Learning For Networks. Proceedings Of The 22nd ACM SIGKDD International Conference On Knowledge Discovery And Data Mining (Pp. 855-864).</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark571">[52] </a><span class="s10">Hou Z, Liu X, Cen Y, Et Al. Graphmae: Self-Supervised Masked Graph Autoencoders[C]//Proceedings Of The 28th ACM SIGKDD Conference On Knowledge Discovery And Data Mining. 2022: 594- 604.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark572">[53] </a><span class="s10">Kipf T N, Welling M. Variational Graph Auto-Encoders[J]. Arxiv Preprint Arxiv:1611.07308, 2016.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark573">[54] </a><span class="s10">Bo D, Wang X, Shi C, Et Al. Beyond Low-Frequency Information In Graph Convolutional Networks[C]//Proceedings Of The AAAI Conference On Artificial Intelligence. 2021, 35(5): 3950-3957.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark574">[55] </a><span class="s10">Yang L, Li M, Liu L, Et Al. Diverse Message Passing For Attribute With Heterophily[J]. Advances In Neural Information Processing Systems, 2021, 34: 4751-4763.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark575">[56] </a><span class="s10">He D, Liang C, Liu H, Et Al. Block Modeling-Guided Graph Convolutional Neural Networks[C]//Proceedings Of The AAAI Conference On Artificial Intelligence. 2022, 36(4): 4022-4029.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 156%;text-align: justify;"><a name="bookmark576">[57] </a><span class="s10">Zhu J, Yan Y, Zhao L, Et Al. Beyond Homophily In Graph Neural Networks: Current Limitations And Effective Designs[J]. Advances In Neural Information Processing Systems, 2020, 33: 7793-7804.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[58] <span class="s10">Chien E, Peng J, Li P, Et Al. Adaptive Universal Generalized Pagerank Graph Neural Network[J]. Arxiv Preprint Arxiv:2006.07988, 2020.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 156%;text-align: justify;"><a name="bookmark577">[59] </a><span class="s10">Wang X, Zhu M, Bo D, Et Al. Am-Gcn: Adaptive Multi-Channel Graph Convolutional Networks[C]//Proceedings Of The 26th ACM</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 4pt;padding-left: 43pt;text-indent: 0pt;line-height: 155%;text-align: justify;">SIGKDD International Conference On Knowledge Discovery &amp; Data Mining. 2020: 1243-1253.</p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[60] <span class="s10">Pei H, Wei B, Chang K C C, Et Al. Geom-Gcn: Geometric Graph Convolutional Networks[J]. Arxiv Preprint Arxiv:2002.05287, 2020.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark578">[61] </a><span class="s10">Lim D, Hohne F, Li X, Et Al. Large Scale Learning On Non- Homophilous Graphs: New Benchmarks And Strong Simple Methods[J]. Advances In Neural Information Processing Systems, 2021, 34: 20887-20902.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark579">[62] </a><span class="s10">Yu Z, Jin D, Wei J, Et Al. Teko: Text-Rich Graph Neural Networks With External Knowledge[J]. IEEE Transactions On Neural Networks And Learning Systems, 2023</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[63] <span class="s10">Jin D, Song X, Yu Z, Et Al. Bite-Gcn: A New Gcn Architecture Via Bidirectional Convolution Of Topology And Features On Text-Rich Networks[C]//Proceedings Of The 14th ACM International Conference On Web Search And Data Mining. 2021: 157-165.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark580">[64] </a><span class="s10">Yu Z, Jin D, Liu Z, Et Al. AS-GCN: Adaptive Semantic Architecture Of Graph Convolutional Networks For Text-Rich Networks[C]//2021 IEEE International Conference On Data Mining (ICDM). IEEE, 2021: 837-846.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 156%;text-align: justify;"><a name="bookmark581">[65] </a><span class="s10">Wang X, Ji H, Shi C, Et Al. Heterogeneous Graph Attention Network[C]//The World Wide Web Conference. 2019: 2022-2032.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[66] <span class="s10">Fu X, Zhang J, Meng Z, Et Al. Magnn: Metapath Aggregated Graph Neural Network For Heterogeneous Graph Embedding[C]//Proceedings Of The Web Conference 2020. 2020: 2331-2341.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 156%;text-align: justify;">[67] <span class="s10">Yun S, Jeong M, Kim R, Et Al. Graph Transformer Networks[J]. Advances In Neural Information Processing Systems, 2019, 32.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s137" style="padding-top: 4pt;padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark582">[68] </a><span class="s10">Jin D, Huo C, Liang C, Et Al. Heterogeneous Graph Neural Network Via Attribute Completion[C]//Proceedings Of The Web Conference 2021. 2021: 391-400.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark583">[69] </a><span class="s10">Jin G, Liang Y, Fang Y, Et Al. Spatio-Temporal Graph Neural Networks For Predictive Learning In Urban Computing: A Survey[J]. Arxiv Preprint Arxiv:2303.14483, 2023.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark584">[70] </a><span class="s10">Chen C, Cheng Z, Li Z, Et Al. Hypergraph Attention Networks[C]//2020 IEEE 19th International Conference On Trust, Security And Privacy In Computing And Communications (Trustcom). IEEE, 2020: 1560-1565.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark585">[71] </a><span class="s10">Kajino H. Molecular Hypergraph Grammar With Its Application To Molecular Optimization[C]//International Conference On Machine Learning. PMLR, 2019: 3183-3191.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark586">[72] </a><span class="s10">Wang C, Pan S, Long G, Et Al. Mgae: Marginalized Graph Autoencoder For Graph Clustering[C]//Proceedings Of The 2017 ACM On Conference On Information And Knowledge Management. 2017: 889-898.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[73] <span class="s10">Hou Z, Liu X, Cen Y, Et Al. Graphmae: Self-Supervised Masked Graph Autoencoders[C]//Proceedings Of The 28th ACM SIGKDD Conference On Knowledge Discovery And Data Mining. 2022: 594- 604.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark587">[74] </a><span class="s10">Kipf T N, Welling M. Variational Graph Auto-Encoders[J]. Arxiv Preprint Arxiv:1611.07308, 2016.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark588">[75] </a><span class="s10">Srong Y, Bian Y, Xu T, Et Al. Self-Supervised Graph Transformer On Large-Scale Molecular Data[J]. Advances In Neural Information Processing Systems, 2020, 33: 12559-12571.</span></p><p class="s137" style="padding-left: 13pt;text-indent: 0pt;text-align: justify;"><a name="bookmark589">[76] </a><span class="s10">Sun K, Lin Z, Zhu Z. Multi-Stage Self-Supervised Learning For</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 4pt;padding-left: 43pt;text-indent: 0pt;line-height: 155%;text-align: justify;">Graph Convolutional Networks On Graphs With Few Labeled Nodes[C]//Proceedings Of The AAAI Conference On Artificial Intelligence. 2020, 34(04): 5892-5899.</p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark590">[77] </a><span class="s10">Peng Z, Dong Y, Luo M, Et Al. Self-Supervised Graph Representation Learning Via Global Context Prediction[J]. Arxiv Preprint Arxiv:2003.01604, 2020.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark591">[78] </a><span class="s10">Hu Z, Kou G, Zhang H, Et Al. Rectifying Pseudo Labels: Iterative Feature Clustering For Graph Representation Learning[C]//Proceedings Of The 30th ACM International Conference On Information &amp; Knowledge Management. 2021: 720- 729.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[79] <span class="s10">Perozzi B, Al-Rfou R, Skiena S. Deepwalk: Online Learning Of Social Representations[C]//Proceedings Of The 20th ACM SIGKDD International Conference On Knowledge Discovery And Data Mining. 2014: 701-710.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[80] <span class="s10">Grover A, Leskovec J. Node2vec: Scalable Feature Learning For Networks[C]//Proceedings Of The 22nd ACM SIGKDD International Conference On Knowledge Discovery And Data Mining. 2016: 855-864.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 156%;text-align: justify;">[81] <span class="s10">Tang J, Qu M, Wang M, Et Al. Line: Large-Scale Information Network Embedding[C]//Proceedings Of The 24th International Conference On World Wide Web. 2015: 1067-1077.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[82] <span class="s10">Zbontar J, Jing L, Misra I, Et Al. Barlow Twins: Self-Supervised Learning Via Redundancy Reduction[C]//International Conference On Machine Learning. PMLR, 2021: 12310-12320.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 156%;text-align: justify;">[83] <span class="s10">Bardes A, Ponce J, Lecun Y. Vicreg: Variance-Invariance- Covariance Regularization For Self-Supervised Learning[J]. Arxiv</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 4pt;padding-left: 43pt;text-indent: 0pt;text-align: justify;">Preprint Arxiv:2105.04906, 2021.</p><p class="s137" style="padding-top: 8pt;padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark592">[84] </a><span class="s10">Zhang H, Wu Q, Yan J, Et Al. From Canonical Correlation Analysis To Self-Supervised Graph Neural Networks[J]. Advances In Neural Information Processing Systems, 2021, 34: 76-89.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark593">[85] </a><span class="s10">Garrido Q, Chen Y, Bardes A, Et Al. On The Duality Between Contrastive And Non-Contrastive Self-Supervised Learning[J]. Arxiv Preprint Arxiv:2206.02574, 2022.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark594">[86] </a><span class="s10">Haochen J Z, Wei C, Gaidon A, Et Al. Provable Guarantees For Self- Supervised Deep Learning With Spectral Contrastive Loss[J]. Advances In Neural Information Processing Systems, 2021, 34: 5000-5011.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[87] <span class="s10">Balestriero R, Lecun Y. Contrastive And Non-Contrastive Self- Supervised Learning Recover Global And Local Spectral Embedding Methods[J]. Advances In Neural Information Processing Systems, 2022, 35: 26671-26685.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;">[88] <span class="s10">Kipf T N, Welling M. Variational Graph Auto-Encoders[J]. NIPS, 2016.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark595">[89] </a><span class="s10">Simonovsky, M., Komodakis, N. Graphvae: Towards Generation Of Small Graphs Using Variational Autoencoders[C]. Artificial Neural Networks And Machine Learning, 2018.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark596">[90] </a><span class="s10">Hongwei Wang, Jia Wang, Jialin Wang And Et Al. Graphgan: Graph Representation Learning With Generative Adversarial Nets[C]. AAAI, 2018.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark597">[91] </a><span class="s10">You J, Ying R, Ren X And Et Al. Graphrnn: Generating Realistic Graphs With Deep Auto-Regressive Models[C]. ICML, 2018.</span></p><p class="s137" style="padding-left: 43pt;text-indent: -30pt;line-height: 156%;text-align: justify;"><a name="bookmark598">[92] </a><span class="s10">Youzhi Luo, Keqiang Yan, Shuiwang Ji. Graphdf: A Discrete Flow Model For Molecular Graph Generation[C]. ICML, 2021.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s137" style="padding-top: 4pt;padding-left: 43pt;text-indent: -30pt;line-height: 155%;text-align: justify;"><a name="bookmark599">[93] </a><span class="s10">Wenqi Fan, C. Liu, Yunqing Liu And At El. Generative Diffusion Models On Graphs: Methods And Applications. IJCAI, 2023.</span></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part328.htm">&lt; 上一个</a><span> | </span><a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8E%9F%E7%90%86.html">内容</a><span> | </span><a href="part330.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
