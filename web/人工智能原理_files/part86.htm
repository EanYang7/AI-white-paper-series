<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-cn" lang="zh-cn"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>5.2.2 分析视角</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part85.htm">&lt; 上一个</a><span> | </span><a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8E%9F%E7%90%86.html">内容</a><span> | </span><a href="part87.htm">下一个 &gt;</a></p><p class="s14" style="padding-left: 7pt;text-indent: 0pt;line-height: 22pt;text-align: left;"><a name="bookmark88">5.2.2 </a><span class="h4">分析视角</span></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">由于神经网络是一个复杂的系统，神经网络的数学分析需要结合多种技术。目前神经网络的数学分析视角主要包括优化理论、概率统计、函数逼近论、信息论和控制论等。</p><p class="s10" style="padding-left: 34pt;text-indent: 0pt;text-align: left;">1.<span class="p">优化理论</span></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">优化理论是研究神经网络的数学原理的重要的分析视角，因为神经网络的训练过程常常被封装成为求解一个最小化目标函数优化问题。优化理论的作用主要包括：</p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">（<span class="s10">1</span>）提供优化速率保证。目前训练神经网络的优化器包含大量超参数，其取值通常基于经验而缺乏理论保障，使得神经网络的表现有较大的不确定性。优化理论通过为优化算法提供收敛速率保证，为优化器的超参选择提供指导，提高神经网络训练过程的稳定性。</p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">（<span class="s10">2</span>）设计新型优化器。收敛速率分析理论能比较不同优化器收敛速度的差异并探讨差异对应的原因，引导设计收敛速度更快的优化器，从理论上指引下一代优化器的发展方向，降低神经网络的训练成本。</p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">（<span class="s10">3</span>）有助于建立泛化理论。优化器本身对于求解目标函数最优解具有偏好，不同偏好往往带来不同的泛化性能，这被称为优化器的隐式正则效应。优化理论可以刻画隐式正则效应，并进一步指导具有更强隐式正则效应的优化器的设计。</p><p class="s10" style="padding-left: 34pt;text-indent: 0pt;text-align: left;">2.<span class="p">概率统计</span></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">概率统计是神经网络的数学原理研究中的另一个重要视角，能分析数据、建模、训练、推理各个环节的随机因素。概率统计的作用主要包括：</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">（<span class="s10">1</span>）刻画数据的属性。概率统计能刻画训练数据和真实场景数据分布之间的距离，为神经网络的场景泛化能力提供基础；而且概率统计可以通过数据建模删除冗余信息，凸显数据特性，启发设计对应的神经网络结构。</p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">（<span class="s10">2</span>）分析训练中的随机性。神经网络训练中，优化器需要随机选取一部分数据进行模型迭代以降低计算开销。概率统计可以分析初始化及优化中的随机性，设计更好的初始化方式，缩减随机性所带来的训练不稳定。</p><p class="s10" style="padding-left: 34pt;text-indent: 0pt;text-align: left;">3.<span class="p">函数逼近论</span></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">在神经网络的数学原理中，函数逼近论被用来分析神经网络的表达能力，即在最理想的数据、算法、算例情况下，神经网络是否有能力完成某一任务。函数逼近论的作用主要包括：</p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">（<span class="s10">1</span>）显示神经网络强大的表达能力。由函数逼近论，神经网络可以高效逼近任意复杂度的连续函数，具有强大的模型表达能力，为实际场景中训练神经网络提供了理论上限，也为神经网络在人工智能任务中取得优秀表现提供了理论依据。</p><p style="padding-left: 34pt;text-indent: 0pt;text-align: left;">（<span class="s10">2</span>）指导神经网络结构的设计。函数逼近论刻画神经网络结构</p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: justify;">（如层数、每层神经元个数、模型基本单元结构）对模型表达能力的影响，能指导如何平衡神经网络的结构参数实现表达能力最大化，并启发针对特定任务设计新的基本架构。</p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">除上述视角以外，信息论与控制论也对分析神经网络的数学原理有重要作用。信息论中的相对熵可以度量两个分布之间的距离，检验参数与数据独立性，设计神经网络的损失函数，基于复杂度度量进行网络压缩等；控制论可以刻画智能体与外界交互的行为，支撑基于神经网络的深度强化学习算法的设计与分析。</p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part85.htm">&lt; 上一个</a><span> | </span><a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8E%9F%E7%90%86.html">内容</a><span> | </span><a href="part87.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
