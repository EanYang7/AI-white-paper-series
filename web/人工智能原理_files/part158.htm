<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-cn" lang="zh-cn"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>8.3.1 传统机器学习问题的量子化</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part157.htm">&lt; 上一个</a><span> | </span><a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8E%9F%E7%90%86.html">内容</a><span> | </span><a href="part159.htm">下一个 &gt;</a></p><p class="s14" style="padding-left: 7pt;text-indent: 0pt;line-height: 22pt;text-align: left;"><a name="bookmark169">8.3.1 </a><span class="h4">传统机器学习问题的量子化</span></p><p class="s10" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">8.3.1.1 <span class="s37">精确学习</span></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">在这个问题设置下，我们假设学习的目标是一个布尔函数，即要从一个已知的函数集合<span class="s11">𝒞 </span>（称作概念类）中学习某个未知的<span class="s11">𝑐 ∶</span></p><p class="s11" style="padding-left: 7pt;text-indent: 0pt;text-align: justify;">{0,1}<span class="s19">𝑛</span> → {0,1}<span class="p">。如同精确学习所表示的那样，在给定成员查询</span></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: justify;">（<span class="s10">Membership Query</span>）下，即当我们向系统查询<span class="s11">𝑥</span>时会得到<span class="s11">𝑐(𝑥)</span>，我们的目标是以大概率精确地确定<span class="s11">𝑐</span>。一般地来说，当衡量算法复杂度的方式是对系统的查询次数时，量子算法对比经典算法会有多项式时间的加速；而当衡量算法复杂度的方式是时间复杂度时，在合理的复杂</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 34pt;text-indent: -28pt;line-height: 139%;text-align: left;">性假设下，对某些学习目标量子算法可以比经典算法指数级别地快。在经典精确学习模型下，给定成员查询<span class="s11">𝑀𝑄(𝑐)</span>，在查询<span class="s11">𝑥</span>时，</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: left;"><span class="s11">𝑀𝑄(𝑐)</span>返回对应的标签<span class="s11">𝑐(𝑥)</span>。称<span class="s11">𝒜</span>是一个对概念类<span class="s11">𝒞</span>的学习者，若对任意的<span class="s11">𝑐 ∈ 𝒞</span>，给定<span class="s11">𝑐</span>对应的成员查询<span class="s11">𝑀𝑄(𝑐)</span>，以至少<span class="s11">2/3</span>的概率<span class="s11">𝒜</span>输出<span class="s11">ℎ</span>，满足<span class="s11">ℎ(𝑥) = 𝑐(𝑥)</span>对任意的<span class="s11">𝑥 ∈ {0, 1}</span><span class="s19">𝑛</span>成立。这个模型也被称作谕示机确认（<span class="s10">Oracle Identification</span>），因为可以把概念类<span class="s11">𝒞</span>看作是一组谕示机的集合，而学习的目标就是在给定某个概念类中喻示机时，确定给定的谕示机是这个概念类<span class="s11">𝒞</span>中的哪一个。</p><p class="s11" style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;"><span class="p">学习者</span>𝒜<span class="p">的查询复杂度是指对所有</span>𝑐 ∈ 𝒞<span class="p">和所有</span>𝒜<span class="p">内部的随机比特串中，学习算法需要的对成员查询</span>𝑀𝑄(𝑐)<span class="p">的最多的查询次数。而对一个概念类</span>𝒞<span class="p">精确学习的查询复杂度是指所有对概念类</span>𝒞<span class="p">的学习者</span>𝒜<span class="p">中需要的最少的查询复杂度。除此之外，由于每个概念</span>𝑐 ∶ {0, 1}<span class="s19">𝑛</span> →</p><p class="s11" style="padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: justify;">{0, 1}<span class="p">都可以由其真值表，一个长度为</span>𝑁 = 2<span class="s19">𝑛</span><span class="p">的比特串唯一确定。给定正整数</span>𝑁<span class="p">和</span>𝑀<span class="p">，我们定义</span>(𝑁, 𝑀)<span class="s10">- </span><span class="p">查询复杂度为所有概念类</span>𝒞 ⊆</p><p class="s11" style="padding-left: 34pt;text-indent: -28pt;line-height: 139%;text-align: justify;">{0, 1}<span class="s19">𝑁</span><span class="p">和</span>|𝒞| = 𝑀<span class="p">中具有最大查询复杂度的概念类对应的查询复杂度。而在量子的场景下，我们允许学习者使用量子算法，并且对应访</span></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 140%;text-align: left;">问的成员查询从<span class="s11">𝑀𝑄(𝑐)</span>变为对应的量子版本<span class="s11">𝑄𝑀𝑄(𝑐)</span>：这个喻示机执行线性可逆变换<span class="s11">|𝑥, 𝑏⟩ ↦ |𝑥, 𝑏 ⊕ 𝑐(𝑥)⟩</span>。对于给定的概念类<span class="s11">𝒞</span>，正整数</p><p class="s11" style="padding-left: 7pt;text-indent: 0pt;line-height: 18pt;text-align: left;">𝑁<span class="p">、</span>𝑀<span class="p">，我们也可以类似地定义精确学习</span>𝒞<span class="p">的量子查询复杂度和</span>(𝑁, 𝑀)<span class="s10">-</span></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">量子查询复杂度。</p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: left;">为了刻画精确学习概念类<span class="s11">𝒞</span>的查询复杂度和量子查询复杂度，我们首先需要介绍一个由概念类<span class="s11">𝒞</span>决定的组合参数<span class="s11">𝛾(𝒞)</span>：</p><p class="s11" style="padding-top: 8pt;padding-left: 246pt;text-indent: 0pt;line-height: 16pt;text-align: left;">|{𝑐 ∈ 𝒞<span class="s51">′</span><span class="p"> </span>∶ 𝑐<span class="s17">𝑖</span> = 𝑏}|</p><p style="text-indent: 0pt;text-align: left;"><span><img width="151" height="1" alt="image" src="Image_058.png"/></span></p><p class="s11" style="padding-left: 69pt;text-indent: 0pt;line-height: 12pt;text-align: left;">𝛾(𝒞) =       min       max min</p><p class="s16" style="padding-left: 116pt;text-indent: 0pt;line-height: 92%;text-align: left;"><span class="s41">𝒞</span><span class="s110">′</span><span class="s41">⊆𝒞,|𝒞</span><span class="s110">′</span><span class="s41">|≥2 </span>𝑖∈<span class="s59">[</span>𝑁<span class="s59">] </span>𝑏∈<span class="s59">{</span>0,1<span class="s59">}</span></p><p class="s11" style="padding-left: 42pt;text-indent: 0pt;line-height: 18pt;text-align: left;">|𝒞<span class="s51">′</span>|</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 27pt;line-height: 140%;text-align: left;">这个看起来很复杂的定义事实上由这个学习程序所启发：如果学习者想要精确地从<span class="s11">𝒞</span>中确定<span class="s11">𝑐</span>，它可以每次贪心地查询真值表中最有价</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: right;">值的一位<span class="s11">𝑖 ∈ [𝑁]</span>，即这一位可以在最坏情况下尽可能多地从目前已经确定<span class="s11">𝑐</span>所在的概念类子集<span class="s11">𝒞</span><span class="s51">′中去除一部分，从而缩小</span><span class="s19">𝒞</span><span class="s51">′并最终确定</span></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: right;"><span class="s11">𝑐</span>。不难看出这个经典算法的查询复杂度是<span class="s11">𝑂(log|𝒞| /𝛾(𝒞))</span>。于是对任意概念类<span class="s11">𝒞</span>，其精确学习的查询复杂度一个上界是<span class="s11">𝑂(log|𝒞| /𝛾(𝒞))</span>。 <span class="s111">2004 </span>年，<span class="s111">Servedio </span>等人在<span class="s111">Bshouty </span>工作的基础上<span class="s112">[78, 79]</span>证明了对</p><p style="text-indent: 0pt;text-align: right;">于任意的概念类<span class="s11">𝒞 </span>， 精确学习的查询复杂度至少为<span class="s11">Ω(max{1/</span></p><p class="s11" style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: justify;">𝛾(𝒞), log|𝒞|})<span class="p">。综上所述，对于精确学习某个概念类的经典查询复杂度我们得到了较紧的上下界。</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="38" height="1" alt="image" src="Image_059.png"/></span></p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;"><span class="s10">2004 </span>年，<span class="s10">Servedio </span>等人<span class="s20">[78]</span>证明了对于任意的概念类<span class="s11">𝒞</span>，精确学习的量子查询复杂度至少为<span class="s11">Ω (max{1/√𝛾(𝒞), log|𝒞| /𝑛})</span>。一些例子表明这个下界是最优的。除此之外，<span class="s111">2013 </span>年，<span class="s111">Kotthari </span><span class="s112">[80]</span>在 <span class="s111">Servedio</span>和<span class="s111">Atici </span>等人工作<span class="s112">[78, 81]</span>的基础上解决了<span class="s111">Hunziker </span>在 <span class="s111">2010 </span>年提出的一个猜想<span class="s112">[82]</span>，证明了精确学习概念类<span class="s11">𝒞</span>的量子查询复杂度一个上界是</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 170pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="99" height="1" alt="image" src="Image_060.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="98" height="1" alt="image" src="Image_061.png"/></span></p><p class="s11" style="text-indent: 0pt;line-height: 13pt;text-align: right;">𝑂 (√</p><p class="s11" style="padding-top: 1pt;padding-left: 13pt;text-indent: 0pt;text-align: left;">1/𝛾(𝒞)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 15pt;text-indent: 0pt;line-height: 13pt;text-align: left;">log|𝒞|)</p><p class="s11" style="padding-left: 3pt;text-indent: 0pt;line-height: 15pt;text-align: center;">log(1/𝛾(𝒞))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">综上所述，我们知道量子对精确学习某个概念类这个问题上至多有多项式级别加速。更精确地，对于某个概念类<span class="s11">𝒞</span>，定义其精确学习的经典和量子查询复杂度分别为<span class="s11">𝐷(𝒞)</span>和<span class="s11">𝑄(𝒞)</span>，更进一步，<span class="s111">Servedio </span>等人在<span class="s112">[78]</span>中还证明了一个推论<span class="s11">𝐷(𝒞) = 𝑂(𝑛𝑄(𝒞)</span><span class="s19">3</span><span class="s11">)</span>。</p><p style="text-indent: 0pt;text-align: left;"><span><img width="16" height="1" alt="image" src="Image_062.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="231" height="1" alt="image" src="Image_063.png"/></span></p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">接下来关注<span class="s11">(𝑁, 𝑀)</span><span class="s10">-</span>查询复杂度和量子查询复杂度之间的关系。经典学习理论中的一个公认的结果是<span class="s11">(𝑁, 𝑀) </span><span class="s10">- </span>查询复杂度是 <span class="s11">Θ(min{𝑁, 𝑀})</span>。至于量子的情况，在<span class="s10">Alon </span>等人、<span class="s10">Ambainis </span>等人和<span class="s10">Iwama</span>等人工作<span class="s20">[83-85]</span>的基础上，<span class="s10">Kothari </span>在 <span class="s10">2013 </span>年<span class="s20">[80]</span>证明了<span class="s11">(𝑁, 𝑀)</span><span class="s10">-</span>量子查询复杂度分为两类， 当<span class="s11">𝑀 ≤ 𝑁 </span>时是<span class="s11">Θ(√𝑀)</span>，而当<span class="s11">𝑁 &lt; 𝑀 ≤ 2</span><span class="s19">𝑁</span><span class="s11"> </span>时是 <span class="s11">Θ(√𝑁 log 𝑀 /(log(𝑁/ log 𝑀) + 1))</span>。这说明在考虑固定参数情况下的最坏采样复杂度时，量子比经典有多项式级别的加速。</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">8.3.1.2 <span class="s37">概率近似正确学习</span></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: left;">在上面精确学习的场景里，我们可以根据自己的需要对不同样本点对应的标签进行查询，在这样非常可控的查询能力下，在查询次数足够多时总是能确定目标函数。在 <span class="s10">PAC </span>学习中我们关心另一个更贴近现实的场景，在这里我们无法完全控制获得的标签对应的样本点，而只能保证每个数据都是从某个未知的分布中独立采样得到的。</p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">在现实的场景中，通常我们收集数据，而后再对数据进行处理和学习，在大部分情况下我们无法根据学习算法的需要对数据集进行补充，只能根据已经采集到的数据尽可能地逼近目标函数。因此相较于精确学习，<span class="s10">PAC </span>学习模型更能抓住这种场景的关键，从而更具有研究价值，受到研究者的广泛关注。</p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">在经典 <span class="s10">PAC </span>学习模型下，学习算法<span class="s11">𝒜</span>能够访问一个能够查询一个随机采样的喻示机<span class="s11">𝑃𝐸𝑋(𝑐, 𝐷)</span>，其中<span class="s11">𝑐 ∈ 𝒞</span>是要学习的目标概念，而</p><p class="s11" style="padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: justify;">𝐷<span class="p">是一个</span>{0,1}<span class="s19">𝑛</span><span class="p">上的未知概率分布。每当学习算法从</span>𝑃𝐸𝑋(𝑐, 𝐷)<span class="p">中查询时，喻示机会根据</span>𝐷<span class="p">产生一个样本</span>𝑥<span class="p">，并返回</span>(𝑥, 𝑐(𝑥))<span class="p">。在这样的背景下，我们称学习算法</span>𝒜<span class="p">是一个对概念类</span>𝒞<span class="p">的</span>(𝜖, 𝛿)<span class="s10">-PAC </span><span class="p">学习者，若对任意的概念</span>𝑐 ∈ 𝒞<span class="p">和概率分布</span>𝐷<span class="p">，学习算法</span>𝒜<span class="p">在给定喻示机</span>𝑃𝐸𝑋(𝑐, 𝐷)<span class="p">的情况下能够以至少</span>1 − 𝛿 <span class="p">的概率输出一个函数</span>ℎ <span class="p">，满足</span>Pr[ℎ(𝑥) ≠</p><p class="s11" style="padding-left: 7pt;text-indent: 0pt;line-height: 140%;text-align: justify;">𝑐(𝑥)] ≤ 𝜖<span class="p">，其中</span>𝑥<span class="p">服从概率分布</span>𝐷<span class="p">。此外，由于学习算法输出的函数</span>ℎ<span class="p">不一定是目标概念</span>𝑐<span class="p">本身，因此也不一定落在概念类</span>𝒞<span class="p">中。如果学习算法输出的函数</span>ℎ<span class="p">总是落在概念类</span>𝒞<span class="p">中，则称这个学习算法是适当的。</span></p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">学习算法<span class="s11">𝒜</span>的采样复杂度定义为其在任意目标概念<span class="s11">𝑐 ∈ 𝒞</span>、任意概率分布<span class="s11">𝐷</span>以及一切算法本身随机性下需要访问<span class="s11">𝑃𝐸𝑋(𝑐, 𝐷)</span>的最大次数。除此之外，我们定义一个概念类<span class="s11">𝒞</span>的<span class="s11">(𝜖, 𝛿)</span><span class="s10">-PAC </span>采样复杂度是所有概念类<span class="s11">𝒞</span>的<span class="s11">(𝜖, 𝛿)</span><span class="s10">-PAC </span>学习者的最小的采样复杂度。</p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 140%;text-align: justify;">量子 <span class="s10">PAC </span>学习模型由 <span class="s10">Bshouty </span>和 <span class="s10">Jackson </span>在 <span class="s10">1995 </span>年定义<span class="s20">[86]</span>。在这个模型下，除了学习者可以使用量子算法外，其访问的喻示机也由</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: left;">经典情况的<span class="s11">𝑃𝐸𝑋(𝑐, 𝐷)</span>变为对应的量子情况<span class="s11">𝑄𝑃𝐸𝑋(𝑐, 𝐷)</span>。每当算法访问这个喻示机，其就会恢复一个量子态</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 209pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="40" height="1" alt="image" src="Image_064.png"/></span></p><p class="s11" style="padding-left: 3pt;text-indent: 0pt;line-height: 15pt;text-align: center;">∑        √𝐷(𝑥)|𝑥, 𝑐(𝑥)⟩</p><p class="s16" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: center;">𝑥∈{0,1}<span class="s113">𝑛</span></p><p style="padding-top: 9pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">这样的一个量子态是前面对经典情况的一个自然的量子拓展。很多量子过程可以产生这样的量子态，考虑在给定一些这样的量子态的情况下进行学习的模型是有意义的。一个量子 <span class="s10">PAC </span>学习算法可以访问一些上述量子态的复制，在其上进行测量并返回结果。</p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">在量子情况下，量子学习算法的采样复杂度就定义为在任意目标概念<span class="s11">𝑐 ∈ 𝒞</span>、任意概率分布<span class="s11">𝐷</span>以及一切算法本身随机性需要的最多的上述量子态的数量。与经典的情况类似，我们定义概念类<span class="s11">𝒞</span>的<span class="s11">(𝜖, 𝛿)</span><span class="s10">-</span>量子 <span class="s10">PAC </span>采样复杂度是所有概念类<span class="s11">𝒞</span>的<span class="s11">(𝜖, 𝛿)</span><span class="s10">-</span>量子 <span class="s10">PAC </span>学习者的最小的采样复杂度。</p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 138%;text-align: justify;">与 <span class="s10">PAC </span>学习有密切联系的组合参数是 <span class="s10">Vapnik </span>和<span class="s10">Chervonenkis [</span><span class="s20">87]</span>引入的 <span class="s10">VC </span>维。对一个概念类<span class="s11">𝒞</span>，称集合<span class="s11">𝑆 = {𝑠</span><span class="s15">1</span><span class="s11">, … , 𝑠</span><span class="s15">𝑡</span><span class="s11">}</span>被打乱，当且仅当<span class="s11">{(𝑐(𝑠</span><span class="s17">1</span><span class="s11">), … , 𝑐(𝑠</span><span class="s17">𝑡</span><span class="s11">)) ∶ 𝑐 ∈ 𝒞} = {0, 1}</span><span class="s19">𝑛</span>。概念类<span class="s11">𝒞</span>的 <span class="s10">VC </span>维就定义为能被打乱的最大集合的大小。接下来用字母<span class="s11">𝑑</span>表示概念类的 <span class="s10">VC </span>维。 <span class="s10">Blumer </span>等人在 <span class="s10">1989 </span>年<span class="s20">[88]</span>证明了概念类<span class="s11">𝒞</span>的<span class="s11">(𝜖, 𝛿)</span><span class="s10">-PAC </span>采样复杂度的</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 17pt;text-align: left;">一个下界是</p><p class="s11" style="padding-left: 42pt;text-indent: 0pt;line-height: 11pt;text-align: center;">1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="1" alt="image" src="Image_065.png"/></span></p><p class="s11" style="padding-left: 3pt;text-indent: 0pt;line-height: 78%;text-align: center;">Ω (𝑑/𝜖 + log (<span class="s18">𝛿</span>) /𝜖)</p><p style="padding-top: 5pt;padding-left: 7pt;text-indent: 27pt;text-align: justify;"><span class="s10">2016 </span>年，<span class="s10">Hanneke</span><span class="s20">[89]</span>在 <span class="s10">Simon 2015 </span>年工作<span class="s20">[90]</span>的基础上证明了这</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 25pt;text-align: left;">个下界是最优的，有了这些结论，我们就得到了概念类<span class="s11">𝒞</span>的 <span class="s10">PAC </span>采样复杂度与其 <span class="s10">VC </span>维的紧的关系，即其的<span class="s11">(𝜖, 𝛿)</span><span class="s10">-PAC </span>采样复杂度是</p><p class="s11" style="padding-left: 42pt;text-indent: 0pt;line-height: 11pt;text-align: center;">1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="1" alt="image" src="Image_066.png"/></span></p><p class="s11" style="padding-left: 3pt;text-indent: 0pt;line-height: 78%;text-align: center;">Θ (𝑑/𝜖 + log (<span class="s18">𝛿</span>) /𝜖)</p><p style="padding-top: 5pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">一个自然的研究方向是考虑概念类<span class="s11">𝒞</span>的<span class="s11">(𝜖, 𝛿)</span><span class="s10">-</span>量子 <span class="s10">PAC </span>采样复杂度与其 <span class="s10">VC </span>维之间的关系。令人惊讶的是，在这个学习场景里量子并不比经典情况更有优势。具体地说，在 <span class="s10">2018 </span>年，<span class="s10">Arunachalam </span>等人<span class="s20">[91]</span></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: left;">在 <span class="s10">Atici </span>与 <span class="s10">Servedio </span>和 <span class="s10">Zhang </span>等人工作<span class="s20">[81, 92]</span>的基础上证明了，对<span class="s11">𝛿 ∈ (0, 1/2)</span>和<span class="s11">𝜖 ∈ (0, 1/20)</span>，概念类<span class="s11">𝒞</span>的<span class="s11">(𝜖, 𝛿)</span><span class="s10">-</span>量子 <span class="s10">PAC </span>采样复杂度的一</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 17pt;text-align: left;">个下界是</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 7pt;text-indent: 0pt;text-align: left;">8.3.1.3 <span class="s37">不可知学习</span></p><p class="s11" style="padding-left: 128pt;text-indent: 0pt;line-height: 11pt;text-align: left;">1</p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="1" alt="image" src="Image_067.png"/></span></p><p class="s11" style="padding-left: 7pt;text-indent: 0pt;line-height: 10pt;text-align: left;">Θ ((𝑑 − 1)/𝜖 + log (</p><p class="s11" style="padding-left: 128pt;text-indent: 0pt;line-height: 13pt;text-align: left;">𝛿</p><p class="s11" style="padding-top: 8pt;text-indent: 0pt;text-align: left;">) /𝜖)</p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">不可知学习（<span class="s10">Agnostic Learning</span>）是对 <span class="s10">PAC </span>学习的一个拓展。在 <span class="s10">PAC </span>学习中，我们总是假设查询得到的标签是准确无误地从目标概念产生的<span class="s11">𝑐(𝑥)</span>，而这一点在现实中并不是一个非常合理的假设。我们无法确定收集到的数据标签总是准确无误的。算法获得的样本标签存在错误的情况是不可知学习所关注的。</p><p class="s11" style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;"><span class="p">经典不可知学习模型中，学习算法访问的喻示机是</span>𝐴𝐸𝑋(𝐷)<span class="p">，其中</span>𝐷<span class="p">是一个</span>{0,1}<span class="s19">𝑛+1</span><span class="p">上的未知概率分布。每次访问</span>𝐴𝐸𝑋(𝐷)<span class="p">都会从</span>𝐷<span class="p">中采样一个样本</span>(𝑥, 𝑏)<span class="p">。对于函数</span>ℎ<span class="p">，定义其错误率</span>𝑒𝑟𝑟<span class="s15">𝐷</span>(ℎ) = Pr[ℎ(𝑥) ≠</p><p class="s11" style="padding-left: 7pt;text-indent: 0pt;line-height: 17pt;text-align: justify;">𝑏]<span class="p">，其中</span>(𝑥, 𝑏)<span class="p">服从概率分布</span>𝐷<span class="p">。对于概率分布</span>𝐷<span class="p">和概念类</span>𝒞<span class="p">，我们可以</span></p><p class="s11" style="padding-top: 7pt;padding-left: 3pt;text-indent: 0pt;text-align: center;"><span class="p">定义其中函数能够达到的最优错误率为</span>𝑜𝑝𝑡<span class="s15">𝐷</span>(𝒞) = min{𝑒𝑟𝑟<span class="s15">𝐷</span>(𝑐) ∶ 𝑐 ∈</p><p class="s11" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;line-height: 137%;text-align: justify;">𝒞}<span class="p">。称一个学习算法</span>𝒜<span class="p">是概念类</span>𝒞<span class="p">的</span>(𝜖, 𝛿)<span class="s10">-</span><span class="p">不可知学习者，当且仅当对任意概率分布</span>𝐷<span class="p">，算法在访问</span>𝐴𝐸𝑋(𝐷)<span class="p">的情况下，以概率不低于</span>1 − 𝛿<span class="p">算法输出一个函数</span>ℎ ∈ 𝒞<span class="p">，满足</span>𝑒𝑟𝑟<span class="s15">𝐷</span>(ℎ) ≤ 𝑜𝑝𝑡<span class="s15">𝐷</span>(ℎ) + 𝜖<span class="p">。不难发现当我们限制</span>𝐷<span class="p">满足</span>𝑜𝑝𝑡<span class="s15">𝐷</span>(𝒞) = 0<span class="p">这就是 </span><span class="s10">PAC </span><span class="p">学习模型。</span></p><p style="padding-left: 34pt;text-indent: 0pt;line-height: 17pt;text-align: justify;">与前文类似，我们可以定义算法<span class="s11">𝒜 </span>的采样复杂度是其访问</p><p class="s11" style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: justify;">𝐴𝐸𝑋(𝐷)<span class="p">的最大可能次数。我们定义概念类</span>𝒞<span class="p">的</span>(𝜖, 𝛿)<span class="s10">-</span><span class="p">不可知采样复杂度是其所有</span>(𝜖, 𝛿)<span class="s10">-</span><span class="p">不可知学习者的最小采样复杂度。</span></p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">量子化版本的不可知学习问题由<span class="s10">Arunachalam </span>和<span class="s10">de Wolf </span>在 <span class="s10">2017</span>年首先研究<span class="s20">[91]</span>。对于概率分布<span class="s11">𝐷</span>，算法可以访问喻示机<span class="s11">𝑄𝐴𝐸𝑋(𝐷)</span>。对这个喻示机的每次查询将会返回量子态</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 226pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="57" height="1" alt="image" src="Image_068.png"/></span></p><p class="s11" style="padding-left: 3pt;text-indent: 0pt;line-height: 15pt;text-align: center;">∑              √𝐷(𝑥, 𝑏)|𝑥, 𝑏⟩</p><p class="s16" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(𝑥,𝑏)∈{0,1}<span class="s113">𝑛+1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 34pt;text-indent: 0pt;text-align: left;">类似地，我们可以定义量子学习者的查询复杂度和概念类<span class="s11">𝒞</span>的</p><p class="s11" style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">(𝜖, 𝛿)<span class="s10">-</span><span class="p">量子不可知采样复杂度。</span></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">经典不可知采样复杂度的下界由 <span class="s10">Vapnik </span>和 <span class="s10">Chervonenkis </span>在他们引入 <span class="s10">VC </span>维的工作中给出<span class="s20">[87]</span>，也可见 <span class="s10">Simon </span>的工作<span class="s20">[93]</span>，上界则由 <span class="s10">Telegrand </span>在 <span class="s10">1994 </span>年给出<span class="s20">[94]</span>。综合这些结果我们得到，对于任意的概念类<span class="s11">𝒞</span>，其<span class="s11">(𝜖, 𝛿)</span><span class="s10">-</span>不可知采样复杂度是<span class="s11">Θ(𝑑/𝜖</span><span class="s19">2</span><span class="s11"> + log(1/𝛿) /𝜖</span><span class="s19">2</span><span class="s11">)</span>，其中<span class="s11">𝑑</span>是概念类<span class="s11">𝒞</span>的 <span class="s10">VC </span>维。这一结论与前面经典 <span class="s10">PAC </span>采样复杂度的结论被 <span class="s10">Shalev-Shwartz </span>和 <span class="s10">Ben-David </span>在他们的著作《理解机器学习：从理论到算法》中称为 <span class="s10">PAC </span>学习基本定理<span class="s20">[95]</span>。</p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">在量子的情况中，<span class="s10">Arunachalam </span>在 <span class="s10">2018 </span>年<span class="s20">[91]</span>证明了对于任意的概念类<span class="s11">𝒞</span>，<span class="s11">𝛿 ∈ (1,1/2)</span>和<span class="s11">𝜖 ∈ (1,1/10)</span>，设<span class="s11">𝒞</span>的 <span class="s10">VC </span>维为<span class="s11">𝑑</span>，则其<span class="s11">(𝜖, 𝛿)</span><span class="s10">-</span>量子不可知采样复杂度至少是<span class="s11">Ω(𝑑/𝜖</span><span class="s19">2</span><span class="s11"> + log(1/𝛿) /𝜖</span><span class="s19">2</span><span class="s11">)</span>。这个证明与前面 <span class="s10">PAC </span>学习的情况类似。作为结论，与 <span class="s10">PAC </span>学习类似，在不可知学习的模型下考虑采样复杂度，量子学习者并不比经典学习者具有显著优势。</p><p class="s10" style="padding-left: 7pt;text-indent: 0pt;text-align: left;">8.3.1.4 <span class="s37">大整数分解与高效 </span>PAC <span class="s37">学习</span></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">虽然在考虑采样复杂度时，量子算法并不比经典算法具有明显优势。但当我们考虑学习算法的时间复杂度时，我们发现量子比经典具有很强的优势。<span class="s10">Servedio </span>和 <span class="s10">Gortler</span><span class="s20">[78]</span>在 <span class="s10">2004 </span>年的工作指出，假设不存在经典的多项式时间算法分解大整数，那么就存在概念类能够被量子算法多项式时间<span class="s10">PAC </span>学习，但不能被任何经典算法多项式时间<span class="s10">PAC</span>学习；也存在概念类能够被量子算法多项式时间精确学习，但不能被任何经典算法多项式时间精确学习。</p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">除此之外，他们还证明了一个更令人惊讶的结果，揭示了量子算法的神奇之处。他们证明了只需要假设经典单向函数存在，就能够找到概念类能够被量子算法多项式时间精确学习，但不能被任何景点算法多项式时间精确学习。这个结论让我们看到了在经典机器学习问题</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">考虑时间复杂度时的量子优越性。</p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part157.htm">&lt; 上一个</a><span> | </span><a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8E%9F%E7%90%86.html">内容</a><span> | </span><a href="part159.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
