<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-cn" lang="zh-cn"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>4.2.2 基于有模型的强化学习方法</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part75.htm">&lt; 上一个</a><span> | </span><a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8E%9F%E7%90%86.html">内容</a><span> | </span><a href="part77.htm">下一个 &gt;</a></p><p class="s14" style="padding-left: 7pt;text-indent: 0pt;line-height: 22pt;text-align: left;"><a name="bookmark77">4.2.2 </a><span class="h4">基于有模型的强化学习方法</span></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 27pt;line-height: 140%;text-align: left;">有模型的强化学习方法使用已知的环境模型或通过学习建立的环境模型来进行规划和决策，根据模型的来源可分为模型给定法</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 18pt;text-align: left;">（<span class="s10">Given Model</span>）和模型学习法（<span class="s10">Learn Model</span>）。</p><p class="s10" style="padding-top: 7pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: left;"><span class="p">模型给定方法假设已经拥有一个准确的环境模型，该模型描述了环境中的状态转移和与状态</span>-<span class="p">动作对关联的奖励。这些问题的已知模型可以是具体规则、概率转移矩阵等，使得智能体能够根据这些信息得到最佳的行动策略。</span><a href="part328.htm#bookmark527" class="s5">AlphaZero </a><span class="s20">[91]</span> <span class="p">是 </span>DeepMind <span class="p">公司开发的一种人工智能算法。通过自我对弈的方式，</span>AlphaZero <span class="p">采用深度强化学习的方法，成功超越了人类在诸如国际象棋、围棋和将棋等游戏上的表现，代表了模型给定强化学习方法的一种典范。</span>AlphaZero <span class="p">的成功在于其与蒙特卡洛树搜索算法的巧妙结合。蒙特卡洛树搜索是一种在博弈游戏中广泛应用的智能搜索算法。该算法通过大量的随机抽样来估计最优策略， 有效的平衡了两者之间的关系。当蒙特卡洛树搜索与 </span>AlphaZero <span class="p">结合使用时，每次搜索步骤可以借助深度神经网络来评估状态并选择行动，进而产生更优的策略。这种将 </span>AlphaZero <span class="p">与蒙特卡洛树搜索结合的方法已被广泛应用于各种难解问题求解中，因为它能够有效的处理大规模状态空间和决策空间，并从中发现较好的策略。例如，</span>Laterre <a href="part328.htm#bookmark528" class="a">等人</a><span class="s20">[92]</span> <span class="p">提出了一种名为</span>Ranked Reward (R2)<span class="p">的算法，用于在单人游戏中实现自我对弈的训练策略。该算法通过对单个代理在多个游戏中获得的奖励进行排名，创建了一个相对性能度量。特别</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: left;"><span class="p">的，</span>R2 <span class="p">算法在二维和三维装箱问题的实例上，优于通用的蒙特卡洛树搜索、启发式算法和整数规划求解器。基于 </span><a href="part328.htm#bookmark529" class="s5">AlphaGo Zero</a><span class="s20">[93]</span> <span class="p">的强化学习策略，</span>Abe <a href="part328.htm#bookmark530" class="a">等人</a><span class="s20">[94]</span> <span class="p">进一步发展了这种思路，提出了一种名为 </span>CombOptZero <span class="p">的新型学习策略，用于求解最小顶点覆盖、最大割和最大团等问题。</span>Abe <a href="part328.htm#bookmark530" class="a">等人</a><span class="s20">[94]</span> <span class="p">将图组合优化问题转化为马尔可夫决策过程，并将 </span>AlphaZero <span class="p">中的卷积神经网络替换为图神经网络，提高了模型对问题的处理能力。在此基础上，利用蒙特卡洛树搜索训练网络，让智能体依据网络预测的信息来确定最佳策略。</span>Li <a href="part328.htm#bookmark531" class="a">等人</a><span class="s20">[95]</span> <span class="p">将深度学习技术与经典启发式算法的元素相结合，提出了一个新的求解方法。该方法通过训练图卷积网络来估计图中每个顶点属于最优解的概率，然后通过树搜索方法搜索解空间。在布尔可满足性、最大独立集、最小顶点覆盖和最大团等问题上的应用表明，该方法与最先进启发式求解器性能相当。</span>Huang <a href="part328.htm#bookmark532" class="a">等人</a><span class="s20">[96]</span> <span class="p">结合 </span>AlphaGoZero <span class="p">和深度神经网络架构，提出了无需任何先验知识的图着色启发式算法。</span>Wang <a href="part328.htm#bookmark533" class="a">等人</a><span class="s20">[97]</span> <span class="p">提出了 </span>AlphaTSP <span class="p">方法，该方法利用自我对弈强化学习来学习一个策略网络，并结合蒙特卡洛树搜索求解旅行商问题。实验证明，</span>AlphaTSP<span class="p">在解决小规模旅行商实例时优于最近邻和传统蒙特卡洛树搜索方法。 </span>Pierrot <a href="part328.htm#bookmark534" class="a">等人</a><span class="s20">[98]</span> <span class="p">结合了神经程序解释器和 </span>AlphaZero <span class="p">的优点，提出了一种新型的算法，被称为 </span>AlphaNPI<span class="p">。神经程序解释器采用模块化、层次结构和递归等结构性偏差，从而减少了样本复杂性，提高了泛化性能。而 </span>AlphaZero <span class="p">则为其提供了神经网络引导搜索算法。</span>Xu <a href="part328.htm#bookmark535" class="a">等人</a><span class="s20">[99]</span><span class="p">提出了 </span>Zermelo Gamification<span class="p">（</span>ZG<span class="p">）方法。该方法将特定组合优化问题转化为</span>Zermelo <span class="p">游戏，且专门设计了一个神经蒙特卡洛树搜索算法。这一策略为求解组合优化问题提供了新的方法和思路。</span></p><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: left;">模型学习方法主要依赖于与环境的交互，进而理解环境的动态性。具体来说，模型学习方法尝试基于已观察到的状态转移和奖励来估计 环境的未知转移和奖励函数。当模型被学习后，它可以用来模拟环境，</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: justify;">预测未来的状态和奖励。模型学习方法可大致分为参数化方法和非参数化方法。参数化方法指的是学习一个具有特定参数的环境模型，而非参数化方法不假定任何特定的环境模型，直接估计状态转移和奖励函数。一些模型学习方法还利用了深度学习技术，以构建更复杂的环境模型。尽管学习环境模型可以帮助更深入的理解环境动态，从而提升规划和决策的有效性，但在实际应用中，特别是在处理组合优化问题时，模型学习方法仍然处于初级阶段。</p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part75.htm">&lt; 上一个</a><span> | </span><a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8E%9F%E7%90%86.html">内容</a><span> | </span><a href="part77.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
