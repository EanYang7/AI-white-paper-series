<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-cn" lang="zh-cn"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>4.2.1 基于无模型的强化学习方法</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part74.htm">&lt; 上一个</a><span> | </span><a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8E%9F%E7%90%86.html">内容</a><span> | </span><a href="part76.htm">下一个 &gt;</a></p><p class="s14" style="padding-left: 7pt;text-indent: 0pt;line-height: 21pt;text-align: left;"><a name="bookmark76">4.2.1 </a><span class="h4">基于无模型的强化学习方法</span></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">无模型方法在求解难解问题时主要关注于直接寻找最优策略，而不需要对环境进行建模，具体包括基于值函数估计的方法、基于策略估计的方法以及两者结合的 <span class="s10">Actor-Critic </span>方法。</p><p style="padding-left: 34pt;text-indent: 0pt;text-align: left;">基于值函数估计的方法试图在与环境交互的过程中估计出每一</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: left;">状态上每一动作对应的累积奖赏，从而得出最佳策略。典型的基于值函数估计的方法有 <span class="s39">Q</span><span class="s10">-learning </span><a href="part328.htm#bookmark498" class="a">方法</a><span class="s20">[62]</span><span class="s10"> </span>、<span class="s10">Deep </span><span class="s39">Q</span><span class="s10">-Networks</span>（<span class="s10">DQN</span><a href="part328.htm#bookmark499" class="a">）方法</a><span class="s20">[63]</span><span class="s10"> </span><a href="part328.htm#bookmark500" class="a">和时序差分方法</a><span class="s20">[64]</span><span class="s10"> </span>。近年来，基于值函数估计的方法在难解问题求解中得到了广泛应用。例如，<span class="s10">Khalil </span><a href="part328.htm#bookmark501" class="a">等人</a><span class="s20">[65]</span><span class="s10"> </span>提出了一种结合 <span class="s39">Q</span><span class="s10">-learning </span>和图嵌入的框架，用于求解旅行商问题、最小顶点覆盖问题和最大割问题。给定问题实例图 <span class="s39">G</span>，该方法将图 <span class="s39">G </span>作为输入，将部分解（图中部分节点的集合）作为状态 <span class="s39">S</span>，并将图中不属于当前状态 <span class="s39">S </span>的一个节点作为动作 <span class="s39">v</span>。在每个时间步，智能体采取动作 <span class="s39">v</span>，并根据环境反馈的回报奖励 <span class="s39">r </span>来更新 <span class="s39">Q </span>值表<span class="s47">，</span>以估计状态 <span class="s39">S </span>和动作 <span class="s39">v </span>的 <span class="s39">Q</span>值。然后，根据 <span class="s39">Q </span>值表选择具有最大收益的动作来进行决策。这种方法的核心思想是通过使用 <span class="s39">Q</span><span class="s10">-learning </span>算法和图嵌入技术，学习并优化状态和动作之间的 <span class="s39">Q </span>值，从而指导求解难解问题的决策过程。<span class="s10">Cappart</span><a href="part328.htm#bookmark502" class="a">等人</a><span class="s20">[66]</span><span class="s10"> </span>设计了深度强化学习模型来确定与问题相关的决策图变量顺序，该模型可用于最大独立集问题和最大割问题。<span class="s10">Bai </span><a href="part328.htm#bookmark503" class="a">等人</a><span class="s20">[67]</span><span class="s10"> </span>提出了一种图神经网络和深度<span class="s39">Q</span><span class="s10">-learning </span>相结合的检测最大公共子图的方法，该方法使用探索树在两个图中提取相应的子图，并通过 <span class="s10">DQN</span>进行训练以优化子图提取奖励。<span class="s10">Song </span><a href="part328.htm#bookmark504" class="a">等人</a><span class="s20">[68]</span><span class="s10"> </span>提出了策略共同训练的元学习框架，该框架可以将强化学习和模仿学习作为子程序，从而改善单视图的学习模式。<span class="s10">Barrett </span><a href="part328.htm#bookmark505" class="a">等人</a><span class="s20">[69]</span><span class="s10"> </span>提出了结合强化学习和深度图网络的探索性组合优化深度 <span class="s39">Q </span>网络，该方法可以使智能体通过训练数据反馈不断改进解决方案。<span class="s10">Liao </span><a href="part328.htm#bookmark506" class="a">等人</a><span class="s20">[70]</span><span class="s10"> </span>提出了一种深度强化学习方法来求解模拟环境中的全局路由问题，该方法使得智能体能够根据所提出的各种问题产生最优路由策略，并利用了深度强化学习的联合优化机制。<span class="s10">Scavuzzo </span><a href="part328.htm#bookmark507" class="a">等人</a><span class="s20">[71]</span><span class="s10"> </span>提出了一种马尔可夫决策过程范式的泛化，为更一般的分支变量选择、强化学习算法和奖励函数提供了基础。 <span class="s10">Qu </span><a href="part328.htm#bookmark508" class="a">等人</a><span class="s20">[72]</span><span class="s10"> </span>提出了一种新的基于强化学习的分支定界算法。该方法与离线强化学习类似，最初在演示数据上进行训练，以大规模加速学</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: left;"><span class="p">习。随着训练效果的提高，智能体开始逐渐用学到的策略与环境交互。确定演示数据与自生成数据的混合比例是提升算法性能的关键。 </span>Wang <a href="part328.htm#bookmark509" class="a">等人</a><span class="s20">[73]</span> <span class="p">提出了一种智能的深度强化学习算法来求解计算资源分配问题，该方法在 </span>MEC <span class="p">架构中引入基于 </span>DQN <span class="p">的 </span>Software Defined Network<span class="p">（</span>SDN<span class="p">）控制器。在 </span>SDN <span class="p">控制器中，智能体通过与 </span>MEC <span class="p">环境的持续交互可以采取行动并获得相应的奖励。</span>He <a href="part328.htm#bookmark510" class="a">等人</a><span class="s20">[74]</span> <span class="p">开发了一个两阶段的框架来求解复杂调度问题，该方法将强化学习和传统运筹学算法结合在一起来求解调度问题。</span>Jacobs <a href="part328.htm#bookmark511" class="a">等人</a><span class="s20">[75]</span> <span class="p">提出鲁棒优化来求解路径优化问题，该方法通过精确求解内部最小化问题来获得鲁棒解，并应用强化学习来学习外部问题的启发式方法。</span></p><p class="s10" style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;"><span class="p">相比基于值函数估计的方法，基于策略估计的方法不需要显式的估计每个状态或动作对应的 </span><i>Q </i><span class="p">值，而是直接输出下一步动作的概率，根据概率来选取动作。近年来，策略梯度、</span>PPO <span class="p">等基于策略估计的方法也逐渐应用于难解问题求解中。</span>Tang <a href="part328.htm#bookmark512" class="a">等人</a><span class="s20">[76]</span> <span class="p">提出了一种混合整数规划求解器和强化学习相结合的切割平面选择方法，该方法将强化学习智能体作为整数规划算法框架中的子程序，动作则是增加新的切割平面，从而获得良好的收益。</span>Yolcu <a href="part328.htm#bookmark513" class="a">等人</a><span class="s20">[77]</span> <span class="p">提出了一种图神经网络和强化学习相结合的随机局部搜索方法，该方法通过图神经网络计算变量选择启发式策略，并通过强化学习训练一套专门针对不同类别的启发式求解器，从而较好的求解布尔满足性问题。</span>Ma <a href="part328.htm#bookmark514" class="a">等人</a><span class="s20">[78]</span> <span class="p">提出了一种图指针网络和分层强化学习相结合的方法来求解旅行商问题，该方法采用分层强化学习框架和 </span>GPN <span class="p">架构，有效的求解了带时间窗约束的旅行商问题。</span>Nazari <a href="part328.htm#bookmark515" class="a">等人</a><span class="s20">[79]</span> <span class="p">提出一种使用强化学习求解车辆路径问题的端到端框架，该方法训练了一个单一的策略模型，仅通过观察奖励信号和遵循可行性规则，就能为类似规模的问题实例找到接近最优的解决方案。</span>Deudon <a href="part328.htm#bookmark516" class="a">等人</a><span class="s20">[80]</span> <span class="p">将神经网络框架扩展到求解旅行商问题，该方法将城市坐标作为输入，使用强化学习训练神经网络，以</span></p><p class="s10" style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: left;"><span class="p">预测城市排列的分布。</span>Kool <a href="part328.htm#bookmark517" class="a">等人</a><span class="s20">[81]</span> <span class="p">提出了一个基于注意力层的模型并使用强化学习来训练模型，从而求解旅行商问题。</span>Bello <a href="part328.htm#bookmark518" class="a">等人</a><span class="s20">[82]</span> <span class="p">提出了一种循环神经网络和强化学习相结合的求解旅行商问题的方法。该方法在给定一组城市坐标的情况下，训练一个循环神经网络，并以负路径长度作为奖励信号，使用策略梯度方法优化循环神经网络的参数，从而预测不同城市排列的分布。</span>Hu <a href="part328.htm#bookmark519" class="a">等人</a><span class="s20">[83]</span> <span class="p">提出了一种新的三维装箱问题，并将深度强化学习结合求解旅行商问题的方法实现对新型三维装箱问题的求解。</span>Lu <a href="part328.htm#bookmark520" class="a">等人</a><span class="s20">[84]</span> <span class="p">提出一种迭代搜索的方法求解车辆路径规划问题。该方法在同一时间训练多个强化学习策略，并使用不同的状态输入特征。</span>Sun <a href="part328.htm#bookmark521" class="a">等人</a><span class="s20">[85]</span> <span class="p">提出了一种搜索进化策略的网络，该网络策略使用强化学习并将变量选择过程建模为马尔可夫决策过程。</span></p><p class="s10" style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: left;"><span class="p">虽然基于值函数估计的方法和基于策略估计的方法在求解难解问题上获得了较好的效果，然而这些方法均存在明显的不足。基于值函数估计的方法不能直接得到动作值输出，难以扩展到连续动作空间上，并存在高偏差的问题。而基于策略估计函数估计的方法要对大量的轨迹进行采样，而每个轨迹之间的差异可能是巨大的，可能导致高方差和较大梯度噪声问题。为了解决高方差和高偏差之间的矛盾，基于值函数估计的方法和基于策略估计的方法被结合起来形成 </span>Actor- Critic  <span class="p">方法。该方法构造一个全能型的智能体，既能直接输出策略，又能通过价值函数来实时评价当前策略的好坏。</span>Emami <a href="part328.htm#bookmark522" class="a">等人</a><a href="part328.htm#bookmark522" class="s33">[86</a><span class="s20">]</span>   <span class="p">提出了 </span>Sinkhorn <span class="p">策略梯度算法（</span>SPG<span class="p">），并为 </span>SPG <span class="p">算法引入的</span>Actor-Critic<span class="p">神经网络架构，将状态空间的表示学习与 </span>Sinkhorn <span class="p">层解耦。</span>Sinkhorn<span class="p">层产生置换矩阵的连续松弛，使 </span>Actor-Critic  <span class="p">架构可以进行端到端的训练。</span>Chen  <a href="part328.htm#bookmark523" class="a">等人</a><a href="part328.htm#bookmark523" class="s33">[87</a><span class="s20">]</span>   <span class="p">提出了一种车辆路径规划的 </span>NeuRewriter  <span class="p">策略。该策略分解为一个区域选择和一个规则选择模块，每个模块都由 </span>Actor-Critic <span class="p">方法训练的神经网络进行参数化。</span>Malazgirt <a href="part328.htm#bookmark524" class="a">等人</a><a href="part328.htm#bookmark524" class="s33">[88</a><span class="s20">]</span>   <span class="p">提出</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: left;"><span class="p">了一种 </span>TauRieL <span class="p">方法，该方法受 </span>Actor-Critic <span class="p">的启发，采用普通前馈网络来获得策略更新向量，并用该向量来改进生成策略的状态转移矩阵。</span>Cappart <a href="part328.htm#bookmark525" class="a">等人</a><span class="s20">[89]</span> <span class="p">提出一种基于深度强化学习方法来求解约束规划问题，该方法将动态规划作为约束规划和深度强化学习之间的桥梁。 </span>Gao <a href="part328.htm#bookmark526" class="a">等人</a><span class="s20">[90]</span> <span class="p">提出了一种学习局部搜索启发式算法的方法，通过行动者</span>-<span class="p">评论家框架进行训练，迭代改进了车辆路径问题的求解方法。</span></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part74.htm">&lt; 上一个</a><span> | </span><a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8E%9F%E7%90%86.html">内容</a><span> | </span><a href="part76.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
