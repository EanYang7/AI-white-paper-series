<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-cn" lang="zh-cn"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>2.5.2 神经符号系统研究现状</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part54.htm">&lt; 上一个</a><span> | </span><a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8E%9F%E7%90%86.html">内容</a><span> | </span><a href="part56.htm">下一个 &gt;</a></p><p class="s14" style="padding-left: 7pt;text-indent: 0pt;text-align: left;"><a name="bookmark56">2.5.2 </a><span class="h4">神经符号系统研究现状</span></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: left;">神经符号系统研究的相关工作归结为三大类：神经辅助符号 <span class="s10">(learning for reasoning)</span>，符号辅助神经<span class="s10">(reasoning for learning)</span>和神经<span class="s10">-</span>符号<span class="s10">(learning-reasoning)</span>。第一类方法以符号系统为主<span class="s10">,</span>神经系统为辅。它的目的是利用神经网络的计算速度快的优势去辅助用符号系统去<a href="part327.htm#bookmark428" style=" color: black; font-family:仿宋, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; vertical-align: -5pt;">搜索解，使得推理更加高效</a><a href="part327.htm#bookmark428" class="s34">[160] </a><a href="part327.htm#bookmark430" class="s34">[161] [162] </a><a href="part327.htm#bookmark431" class="s34">[163] [164] </a><a href="part327.htm#bookmark432" class="s34">[165] </a><span class="s22">[166] </span><span class="s21">。例如在</span></p><p class="s20" style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: justify;"><span class="p">基于统计关系学习</span><span class="s10">(Statistical Relational Learning,</span><span class="p">简称 </span><a href="part327.htm#bookmark433" class="s5">SRL)</a>[167]<span class="s10"> </span><a href="part327.htm#bookmark434" class="a">的方法中用神经网络来近似搜索算法，实现模型的高效推理。第二类以神经系统为主，符号系统为辅。它的目的是整合符号系统（符号知识包括逻辑规则和知识图谱）的优势到神经系统学习的过程中，提升神经系统的学习性能</a>[168]<a href="part327.htm#bookmark434" class="s5"> </a>[169]<a href="part327.htm#bookmark435" class="s5"> </a>[170]<a href="part327.htm#bookmark435" class="s5"> </a>[171]<a href="part327.htm#bookmark436" class="s5"> </a>[172]<span class="s10"> </span><span class="p">。如正则化</span><span class="s10">(Regularization)</span><span class="p">方法将符号作为神经网络的约束项，指导模型的学习。第三类方法，神经系统与符号系统以平等的方式参与到模型中，两者以一种互利共存</span><a href="part327.htm#bookmark437" style=" color: black; font-family:仿宋, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; vertical-align: -5pt;">的方式充分发挥其优势</a><a href="part327.htm#bookmark437" class="s34">[173] </a><a href="part327.htm#bookmark438" class="s34">[174] [175] </a><span class="s22">[176] </span><span class="s21">。</span></p><p style="padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;">神经辅助符号。<span class="s10">Qu </span>和 <span class="s10">Zhang </span>等人分别提出了概率逻辑神经网络 <span class="s10">(probabilistic Logic Neural Network, </span>简 称 <a href="part327.htm#bookmark439" class="s5">pLogicNet)</a><span class="s20">[177]</span><span class="s10"> </span>和 <a href="part327.htm#bookmark440" class="s5">ExpressGNN</a><span class="s20">[178]</span><span class="s10"> </span>。这两个模型的主要思想都是将知识图谱中的推理问题（三元组补全问题）建模为概率图中隐变量的推理问题，都采用变分 <span class="s10">EM </span>方法和神经网络相结合的思路进行近似推理。以上方法手工设计概率图模型的势函数，但仅利用专家构建的规则不能完全捕获隐含在数据中的知识，为此研究者们开始探索从数据中自动捕获规则的方法。<span class="s10">Marra </span><a href="part327.htm#bookmark441" class="a">等人</a><span class="s20">[179]</span><a href="part327.htm#bookmark441" class="s5"> </a><span class="s20">[180]</span><span class="s10"> </span>扩展了马尔可夫逻辑网（<span class="s10">Markov logic network, </span>简称 <span class="s10">MLN</span>）的学习模型，设计了一个通用的神经网络从原始数据中自动学习规则和 <span class="s10">MLN </span>的势函数。除了基于 <span class="s10">MLN </span>自动学习规则的方法以外，一些学者在传统 <span class="s10">ILP </span>方法的基础上，结合神经网络<span class="s21">与逻辑提出了可微的 </span><a href="part327.htm#bookmark429" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; vertical-align: -5pt;">ILP </a><a href="part327.htm#bookmark429" class="s34">[161] </a><a href="part327.htm#bookmark442" class="s34">[181] [182] </a><a href="part327.htm#bookmark443" class="s34">[183] </a><span class="s22">[184] </span><span class="s21">。</span><span class="s44">Yang </span><a href="part327.htm#bookmark444" style=" color: black; font-family:仿宋, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; vertical-align: -5pt;">等人</a><span class="s22">[185] </span><span class="s21">提</span>出了神经逻辑归纳学习方法<span class="s10">(Neural Logic Inductive Learning, </span>简称 <span class="s10">NLIL)</span>，可以推理复杂的规则信息（如树型规则和合取式规则等），通过学习到的 <span class="s10">FOL </span>解释隐含在数据中的模式，应用于目标检测任务。</p><p class="s10" style="padding-top: 1pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;"><span class="p">符号辅助神经。</span>Diligenti <span class="p">与 </span>Xu <a href="part327.htm#bookmark445" class="a">等人</a><span class="s20">[186]</span><a href="part327.htm#bookmark445" class="s5"> </a><span class="s20">[187]</span> <span class="p">分别提出语义正则化</span>(SBR)<span class="p">和语义损失</span>(SL)<span class="p">方法，将逻辑知识作为假设空间的约束，如果违背了相应的逻辑规范或逻辑理论，模型将受到惩罚。</span>SBR <span class="p">是一种从约束中学习的方法</span>,<span class="p">该方法融合了经典机器学习（具有连续特征表</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: left;"><span class="p">示学习能力）和统计关系学习（具有高级语义知识推理能力），以解决多任务优化和分类等问题。</span>SL  <span class="p">方法将命题逻辑的自动推理技术与现有的深度学习架构相结合，将神经网络的输出喂给损失函数，作为可学习神经网络的约束，通过训练算法将命题逻辑编码为损失函数的一部分，提高神经网络学习能力。该方法使用目标原子的边缘概率定义正则项，并用算术电路评估模型的有效性。</span>Hu  <a href="part327.htm#bookmark446" class="a">等人</a><a href="part327.htm#bookmark446" class="s33">[188</a><span class="s20">]</span>   <span class="p">提出了一个通用框架，称为利用逻辑规则辅助深度学习的方法</span>(Harnessing Deep Neural Networks with Logic Rules<span class="p">，简称 </span>HDNN)<span class="p">。其中，神经网络模型包括 </span>CNN <span class="p">和 </span>DNN <span class="p">等。该方法借助知识蒸馏的思想，通过一个大规模且已训练好的网络去引导小规模网络的学习，设计了教师网络和学生网络。教师网络能够学习标签数据和逻辑规则（无标签数据）中的信息，学生网络近似教师网络，使逻辑规则编码的结构化信息可以约束学生网络的学习。</span>Xie <a href="part327.htm#bookmark447" class="a">等人</a><a href="part327.htm#bookmark447" class="s33">[189</a><span class="s20">]</span>   <span class="p">将命题逻辑融入到关系检测模型中，提出了具有语义正则化的逻辑嵌入网络</span>(Logic Embedding Network with Semantic Regularization<span class="p">，简称 </span>LENSR)<span class="p">，以提升深度模型的关系检测能力。</span>Luo  <a href="part327.htm#bookmark448" class="a">等人</a><a href="part327.htm#bookmark448" class="s33">[190</a><span class="s20">]</span>   <span class="p">提出基于上下文的零次学习方法</span>(Context- Aware Zero-Shot Recognition<span class="p">，简称 </span>CA-ZSL)<span class="p">，解决零次目标检测问题。它基于深度学习和条件随机场建立模型，利用类别之间的语义关系图谱辅助识别未知类别的目标。</span>Li  <a href="part327.htm#bookmark449" class="a">等人</a><a href="part327.htm#bookmark449" class="s33">[191</a><span class="s20">]</span>   <span class="p">提出了大规模的小样本学习方法</span>(Large-scale few shot learning<span class="p">，简称 </span>LSFSL)<span class="p">，解决小样本图像识别任务。</span>Chen  <a href="part327.htm#bookmark450" class="a">等人</a><a href="part327.htm#bookmark450" class="s33">[192</a><span class="s20">]</span>   <span class="p">发现迁移类间的相关性信息可以帮助学习新概念，采用知识图谱建模已知类与未知类之间的相关性，结合神经网络提出了知识图谱迁移网络模型 </span>(Knowledge  Graph  Transfer Network<span class="p">，简称 </span>KGTN)<span class="p">，解决小样本分类问题。</span></p><p class="s10" style="padding-top: 1pt;padding-left: 7pt;text-indent: 27pt;line-height: 139%;text-align: justify;"><span class="p">神经</span>-<span class="p">符号。</span>Robin <a href="part327.htm#bookmark428" class="a">等人</a><span class="s20">[160]</span> <span class="p">基于 </span>ProbLog <span class="p">提出了将概率、逻辑与深度学习有机结合的模型—深度概率逻辑</span>(Deep probabilistic logic<span class="p">，简称 </span>DeepProbLog)<span class="p">。该模型首次提出了将神经网络与概率逻辑以某种</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 139%;text-align: left;"><span class="p">方式结合的通用框架，它兼具两者的优点，有更强的表达能力，并且可以基于样本进行端到端的训练。</span>DeepProbLog <span class="p">是一种概率程序设计语言，以“神经”谓词</span>(neural predicates)<span class="p">的方式与深度学习结合，也就是输入谓词中的实体对（该文中的数据都是二元谓词）。神经网络的分类器预测实体对属于所有谓词的概率，然后将预测的结果用于概率程序的推理。不同于 </span>DeepProbLog<span class="p">，</span>Zhou <a href="part327.htm#bookmark451" class="a">等人</a><a href="part327.htm#bookmark451" class="s33">[193</a><span class="s20">]</span>   <span class="p">基于反绎推理提出了反绎学习（</span>abductive learning<span class="p">，简称 </span>ABL<span class="p">）。</span>ABL <span class="p">利用机器学习的归纳和逻辑程序的反绎，将两者结合到一个统一的框架中，进行端到端的学习。该方法用逻辑程序语言表示一阶谓词逻辑知识并且建模复杂的推理任务。其中，逻辑程序语言中的部分事实由机器学习提供。基于 </span>ABL <span class="p">的思想，</span>Tian <a href="part327.htm#bookmark452" class="a">等人</a><a href="part327.htm#bookmark452" class="s33">[194</a><span class="s20">]</span>   <span class="p">提出了一个弱监督的神经符号学习模型（</span>WS-NeSyL<span class="p">）去解决具有逻辑推理的感知任务。该方法将机器学习部分设计为一个编码器</span>-<a href="part327.htm#bookmark453" class="a">解码器框架，其中包含一个编码器和两个解码器。编码器将输入信息映射为一个向量后，感知解码器会将向量解码为伪标签，认知解码器会基于伪标签采样出来的逻辑规则进行推理。整个过程是一个端到端的迭代优化，直到模型收敛。吉林大学杨博团队</a><a href="part327.htm#bookmark453" class="s33">[195</a><span class="s20">]</span>   <span class="p">提出了一个双层概率图推理框架（</span>BPGR<span class="p">），该框架利用统计关系将神经网络和符号推理整合起来。其中，概率图模型的上层节点是神经网络的预测结果，下层节点是逻辑规则中的闭原子。整个模型进行端到端的迭代训练，直到神经网络的预测结果不再变化。 </span>BPGR <span class="p">用神经网络提升了概率推断的效率，用符号推理提升了神经网络的预测性能。</span></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part54.htm">&lt; 上一个</a><span> | </span><a href="../%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8E%9F%E7%90%86.html">内容</a><span> | </span><a href="part56.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
