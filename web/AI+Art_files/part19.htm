<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>2.4.4 LoRA</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part18.htm">&lt; 上一个</a><span> | </span><a href="../AI%2BArt.html">内容</a><span> | </span><a href="part20.htm">下一个 &gt;</a></p><h4 style="padding-top: 11pt;padding-left: 10pt;text-indent: 0pt;text-align: left;"><a name="bookmark45">2.4.4 LoRA</a><a name="bookmark103">&zwnj;</a></h4><p class="s9" style="padding-top: 7pt;padding-left: 10pt;text-indent: 0pt;text-align: left;"><a name="bookmark46">2.4.4.1 LoRA </a><span class="s18">技术简介</span></p><p style="padding-top: 7pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: left;">随着预训练模型规模的增大，通过全微调 <span class="s9">(full fine-tuning) </span>重新训练所有模型参数变得愈渐困难。例如，使用 <span class="s9">Adam </span>微调的 <span class="s9">GPT-3 175B </span><a href="part19.htm#bookmark104" class="s6">模型</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">23</span>部署独立实例的成本非常高昂，每个实例有 <span class="s9">175B </span>个参数。为了解决这个问题，微软在 <span class="s9">2021 </span>年提出了一种名为低秩自适应</p><p style="padding-left: 10pt;text-indent: 0pt;text-align: left;">（<span class="s9">LoRA</span><a href="part19.htm#bookmark105" class="s6">）的方法</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">24</span>。</p><p style="padding-top: 7pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;"><span class="s9">LoRA </span>的核心思想是冻结预训练模型的权重，并将可训练的秩分解矩阵注入 <span class="s9">Transformer </span>架构的每一层中，这样可以显著减少下游任务需要训练的参数数量。与 <span class="s9">GPT-3 175B </span>相比，<span class="s9">LoRA </span>将可训练参数数量减少 <span class="s9">10000 </span>倍，并且减少了 <span class="s9">GPU </span>内存需求。与 <a href="part19.htm#bookmark106" class="s5">RoBERTa</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">25</span>、 <a href="part19.htm#bookmark107" class="s5">DeBERTa</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">26</span>、<a href="part19.htm#bookmark108" class="s5">GPT-2</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">27</span>和<span class="s9">GPT-3 </span>等模型的微调结果相比，<span class="s9">LoRA </span>在满足更少训练参数和更高吞吐量的同时，能取得与前者相当甚至更优的性能。此外，与适配器方法不同，<span class="s9">LoRA </span>并没有引入额外的推理延迟。</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s21" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">IEEE/CVF conference on computer vision and pattern recognition, pages 10684-10695, 2022.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 10pt;text-indent: 18pt;line-height: 241%;text-align: left;"><a name="bookmark104">23 </a><span class="s21">Brown T., et al. Language models are few-shot learners[J]. Advances in neural information processing systems, 2020, 33: 1877-1901.</span></p><p class="s20" style="padding-left: 10pt;text-indent: 18pt;line-height: 241%;text-align: left;"><a name="bookmark105">24</a><span class="s21"> Hu J. E., et al. Lora: Low-rank adaptation of large language models[J]. arXiv preprint arXiv:2106.09685, 2021.</span></p><p class="s23" style="padding-left: 10pt;text-indent: 18pt;line-height: 242%;text-align: left;"><a name="bookmark106">25 </a><span class="s21">Liu Y., et al. Roberta: A robustly optimized bert pretraining approach[J]. arXiv preprint arXiv:1907.11692, 2019.</span></p><p class="s23" style="padding-left: 10pt;text-indent: 18pt;line-height: 241%;text-align: left;"><a name="bookmark107">26 </a><span class="s21">He P., et al. Deberta: Decoding-enhanced bert with disentangled attention[J]. arXiv preprint arXiv:2006.03654, 2020.</span></p><p class="s20" style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark108">27</a><span class="s21"> Radford A., et al. Language models are unsupervised multitask learners[J]. OpenAI blog, 2019, 1(8): 9.</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="53" height="16" alt="image" src="Image_050.png"/></span></p><p class="s9" style="padding-top: 8pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">LoRA <span class="p">从</span>Li <a href="part19.htm#bookmark109" class="s6">等人</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">28</span><span class="p">和</span>Aghajanyan <a href="part19.htm#bookmark110" class="s6">等人</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">29</span><span class="p">的工作中获得灵感，后者表明，学习的过度参数化模型实际上存在于低内在维度上，所以论文提出了一种低秩自适应 </span>(LoRA) <span class="p">的方法。</span>LoRA <span class="p">允许通过优化适应过程中密集层变化的秩分解矩阵来间接训练神经网络中的一些密集层，同时保持预训练的权重不变。</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="87" height="16" alt="image" src="Image_051.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="70" height="15" alt="image" src="Image_052.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="22" height="10" alt="image" src="Image_053.png"/></span></p><p style="padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: right;">假设给定一个由<span class="s9">Φ</span>参数化的预训练自回归语言模型         ， <span class="s9">LoRA </span>采用了一种有效的参数方法， 令任务特定参数增量由一组更小的参数           进一步编码。因此，求</p><p style="padding-left: 51pt;text-indent: 0pt;text-align: left;">的任务可转变为在<span class="s9">Θ</span>上进行优化：</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="267" height="59" alt="image" src="Image_054.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s9" style="padding-left: 10pt;text-indent: 0pt;text-align: left;"><a name="bookmark47">2.4.4.2 LoRA </a><span class="s18">在</span>Stable Diffusion <span class="s18">上的应用</span></p><p class="s9" style="padding-top: 7pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">LoRA<span class="p">（</span>Low-rank Adaptation<span class="p">）技术提供了一种快速微调扩散模型的方法，使得在不同概念（例如角色或特定风格）上训练 </span>Stable Diffusion <span class="p">模型更加便捷。经过训练的这些模型可以导出并供他人在其自身生成任务中使用。</span></p><p class="s9" style="padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">Stable Diffusion <span class="p">模型因其出色的图像和文本生成能力而在机器学习领域受到广泛关注。然而，这些模型的主要缺点之一是它们的文件大小较大，给个人计算机的维护带来困难。</span>LoRA <span class="p">技术通过引入低秩适应方法，为稳定扩散（</span>Stable Diffusion<span class="p">）模型带来了诸多优势，</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-top: 4pt;padding-left: 10pt;text-indent: 18pt;line-height: 241%;text-align: left;"><a name="bookmark109">28</a><span class="s21"> Li C., et al. Measuring the intrinsic dimension of objective landscapes[J]. arXiv preprint arXiv:1804.08838, 2018.</span></p><p class="s23" style="padding-left: 10pt;text-indent: 18pt;line-height: 242%;text-align: left;"><a name="bookmark110">29 </a><span class="s21">Aghajanyan A., et al. Intrinsic dimensionality explains the effectiveness of language model fine-tuning[J]. arXiv preprint arXiv:2012.13255, 2020.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">特别是在应用于图像与文本生成任务方面。</p><p style="padding-top: 7pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">使用<span class="s9">LoRA </span>技术训练<span class="s9">Stable Diffusion </span>具有几个关键优势：</p><p class="s9" style="padding-top: 7pt;padding-left: 38pt;text-indent: 0pt;text-align: justify;">1. <span class="p">高效的微调过程</span></p><p class="s9" style="padding-top: 7pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">LoRA <span class="p">允许使用低秩适应技术对扩散模型进行快速微调，相较于传统的优化方法，其在微调过程中大幅降低了计算成本和硬件要求。由于</span>LoRA <span class="p">不需要计算所有参数的梯度或维护优化器状态，仅需优化注入的远比原模型小的低秩矩阵，从而大幅简化了微调的计算复杂度。</span></p><p class="s9" style="padding-left: 38pt;text-indent: 0pt;text-align: justify;">2. <span class="p">降低硬件门槛</span></p><p style="padding-top: 7pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">由于稳定扩散模型本身在训练和推理过程中需要较大的计算资源和存储空间，这使得普通用户难以维护一个包含大量模型的收藏。 <span class="s9">LoRA </span>技术的引入有效缩减了模型的文件大小，将其从原来的几 <span class="s9">GB</span>级别降低至 <span class="s9">2-500 MB</span>，使得用户能够更轻松地在个人计算机上运行和应用稳定扩散模型。</p><p class="s9" style="padding-left: 38pt;text-indent: 0pt;text-align: justify;">3. <span class="p">保持模型质量</span></p><p style="padding-top: 7pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">虽然<span class="s9">LoRA </span>将稳定扩散模型文件大小大幅减小，但在此过程中并没有牺牲模型的生成质量。<span class="s9">LoRA </span>技术在文件大小和训练能力之间取得了良好的平衡，使其成为拥有大量模型的用户的理想选择。用户可以获得高质量的图像与文本生成结果，而无需担忧过大的模型参数和计算成本。</p><p class="s9" style="padding-left: 38pt;text-indent: 0pt;text-align: justify;">4. <span class="p">导出与分享</span></p><p style="padding-top: 7pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: left;">经过使用 <span class="s9">LoRA </span>技术微调的稳定扩散模型仍然保持了较高的生成能力和多样性，这使得用户可以轻松导出和分享这些经过优化的模型供他人使用。导出的模型可以用于在不同概念（如角色或特定风格）上进行图像和文本的生成任务，为研究人员和开发者提供了更多实验和应用的可能性。</p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;">总的来说，<span class="s9">LoRA </span>技术在稳定扩散模型中的应用为用户提供了更</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 10pt;text-indent: 0pt;line-height: 139%;text-align: justify;">高效的模型训练和应用体验。通过降低硬件门槛、保持模型质量和方便的模型导出，<span class="s9">LoRA </span>为生成高质量图像和文本的研究和应用带来了更多的便利和可能性。</p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part18.htm">&lt; 上一个</a><span> | </span><a href="../AI%2BArt.html">内容</a><span> | </span><a href="part20.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
