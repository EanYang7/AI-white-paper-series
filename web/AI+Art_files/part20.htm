<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>2.4.5 ControlNet</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part19.htm">&lt; 上一个</a><span> | </span><a href="../AI%2BArt.html">内容</a><span> | </span><a href="part21.htm">下一个 &gt;</a></p><h4 style="padding-top: 1pt;padding-left: 10pt;text-indent: 0pt;text-align: left;"><a name="bookmark48">2.4.5 ControlNet</a><a name="bookmark111">&zwnj;</a></h4><p style="padding-top: 8pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;"><a href="part20.htm#bookmark112" class="s5">ControlNet</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">30</span>是一种控制预训练大型扩散模型的方法，它可以支持额外的输入条件。通过端到端的学习方式，即使在训练数据集很小的情况下，<span class="s9">ControlNet </span>也能够有效地学习特定任务的条件输入<span class="s9">,</span>同时保证训练速度与微调扩散模型相当。<span class="s9">ControlNet </span>能够成功地增强诸如 <span class="s9">Stable Diffusion </span>这类大型扩散模型，使其能够支持边缘图、分割图、关键点等条件输入，丰富了大型扩散模型的控制策略，促进其应用发展。</p><p class="s9" style="padding-left: 38pt;text-indent: 0pt;text-align: left;">1. ControlNet <span class="p">网络设计</span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="10" alt="image" src="Image_055.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="8" height="7" alt="image" src="Image_056.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="10" alt="image" src="Image_057.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="10" alt="image" src="Image_058.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="80" height="16" alt="image" src="Image_059.png"/></span></p><p class="s9" style="padding-top: 7pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: left;">Stable Diffusion <span class="p">模型基本上是一个</span>U-net <span class="p">结构，包含一个编码器、中间块和一个带有跳跃连接的解码器。如果不加</span>ControlNet<span class="p">，扩散模型原始的神经网络    输入    后获得    ，其参数用    表示，则：</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="11" alt="image" src="Image_060.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="15" height="14" alt="image" src="Image_061.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Image_062.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="277" height="15" alt="image" src="Image_063.png"/></span></p><p class="s9" style="padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">ControlNet <span class="p">对神经网络模块的输入条件进行操作，进一步控制网络的整体行为。它将模型原始的网络参数 固定，并复制了一个原始模型分支 ，在此基础上应用了外部条件向量 进行训练，并将施加控制条件的分支通过零卷积和原始模型分支逐层相加。这种设计的优势在于它能够避免在小数据集上过拟合，并保留对数十亿张图像学习的原始大模型能力。加了控制条件之后，将原始网络的输出修改为：</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-left: 10pt;text-indent: 18pt;line-height: 241%;text-align: left;"><a name="bookmark112">30</a><span class="s21"> Rombach R., et al. High-resolution image synthesis with latent diffusion models[C]. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684-10695, 2022.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="11" alt="image" src="Image_064.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="65" height="14" alt="image" src="Image_065.png"/></span></p><p style="padding-top: 3pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">其中零卷积层 <span class="s9">(zero convolution)</span>的初始化权重和偏置都为 <span class="s9">0</span>，两层零卷积的参数为 。</p><p style="padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">需要注意的是，未经训练的 <span class="s9">ControlNet </span>分支输出为零，因此添加到原始网络的数值也为零。这对原始网络没有任何影响，确保了原网络性能的完整保留。</p><p class="s9" style="padding-left: 38pt;text-indent: 0pt;text-align: left;">2. <span class="p">基于稳定扩散的</span>ControlNet</p><p style="padding-top: 7pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">（<span class="s9">1</span>）训练过程如下：</p><p style="padding-top: 7pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">原始的<span class="s9">Stable Diffusion </span>的优化目标如下：</p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;"><span><img width="234" height="18" alt="image" src="Image_066.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="11" alt="image" src="Image_067.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="11" alt="image" src="Image_068.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="7" height="7" alt="image" src="Image_069.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="281" height="18" alt="image" src="Image_070.png"/></span></p><p style="padding-left: 40pt;text-indent: -1pt;line-height: 139%;text-align: left;">在采样     经过网络     进行去噪后，与原始特征经过网络后得到的潜变量计算<span class="s9">L2 </span>损失，而 <span class="s9">ControlNet </span>的训练目标改进为：</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="13" height="11" alt="image" src="Image_071.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="12" height="11" alt="image" src="Image_072.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="14" height="13" alt="image" src="Image_073.png"/></span></p><p style="padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">其中网络 增量了两个控制条件，文字条件 和任务指定的条件 ，例如<span class="s9">Canny </span>边缘图等。</p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;">（<span class="s9">2</span>）<span class="s9">ControlNet </span>的应用如下：</p><p style="padding-top: 7pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: right;"><span class="s9">ControlNet </span>的 推 理 过 程 中 使 用 <a href="part20.htm#bookmark113" class="s5">DDIM</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">31</span><span class="s9">(Denoising Diffusion Implicit Models) </span>的采样方式，默认使用 <span class="s9">20 </span>个时间步长<span class="s9">.</span>在这个推理过程中，用户可以选择不同的 <span class="s9">prompt </span>模式来指导生成图像，其中 <span class="s9">prompt</span>可以为空字符串、默认的“专业、详细、高质量图像”语句、利用 <a href="part20.htm#bookmark114" class="s5">BLIP</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">32</span>等模型自动生成的图像标注，或者用户自定义的词句。实验发现<span class="s9">ControlNet </span>在各种不同条件任务中都能取得较高质量的生成结果。 <span class="s9">Canny </span>边缘检测图。当使用简单的 <span class="s9">Canny </span>边缘检测来提取图像的</p><p style="text-indent: 0pt;text-align: right;">纹理信息时，可以通过这些纹理信息生成各种不同风格的图像，这些</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark113">31 </a><span class="s21">Song J., et al. Denoising diffusion implicit models[J]. arXiv preprint arXiv:2010.02502, 2020.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 10pt;text-indent: 18pt;line-height: 242%;text-align: left;"><a name="bookmark114">32      </a><span class="s21">Li, J., et al. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation[J]. International Conference on Machine Learning. PMLR, 2022.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">图像呈现出逼真而生动的效果。</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 106pt;text-indent: 0pt;text-align: left;"><span><img width="338" height="235" alt="image" src="Image_074.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;text-indent: 0pt;text-align: right;">来源：<span class="s9">ControlNet</span><span class="s28">[4]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-left: 6pt;text-indent: 0pt;text-align: center;">图 <span class="s16">2-5 ControlNet </span>基于 <span class="s16">Canny </span>边缘条件生成结果图</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: left;">人体姿势图。基于学习的姿势估计方法令从人体姿势关键点到自然图像的转换也轻而易举，这类条件控制模型为各种人机交互、动画、虚拟现实等应用领域带来了更广阔的发展空间。</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 53pt;text-indent: 0pt;text-align: left;"><span><img width="469" height="265" alt="image" src="Image_075.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-top: 4pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">图 <span class="s16">2-6 ControlNet </span>基于人体姿势条件生成结果图（来源：<a href="part20.htm#bookmark115" class="s43">ControlNet</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; vertical-align: 4pt;">33</span>）</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark115">33 </a><span class="s21">Ho J., et al. Denoising diffusion probabilistic models[J]. Advances in neural information processing</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">卡通线图。利用网络上的卡通插图提取线描并生成彩色图像， <span class="s9">ControlNet </span>能够从卡通插图中捕捉线条轮廓，然后将其填充上合适的色彩，使得上色过程变得自然而轻松，同时呈现出生动的笔触效果。</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 96pt;text-indent: 0pt;text-align: left;"><span><img width="358" height="342" alt="image" src="Image_076.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-top: 4pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">图 <span class="s16">2-7 ControlNet </span>基于卡通线图条件生成结果图（来源：<a href="part21.htm#bookmark117" class="s43">ControlNet</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; vertical-align: 4pt;">34</span>）</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part19.htm">&lt; 上一个</a><span> | </span><a href="../AI%2BArt.html">内容</a><span> | </span><a href="part21.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
