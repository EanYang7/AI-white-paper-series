<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>2.4.3 Stable Diffusion</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part17.htm">&lt; 上一个</a><span> | </span><a href="../AI%2BArt.html">内容</a><span> | </span><a href="part19.htm">下一个 &gt;</a></p><h4 style="padding-top: 1pt;padding-left: 10pt;text-indent: 0pt;text-align: left;"><a name="bookmark41">2.4.3 Stable Diffusion</a><a name="bookmark91">&zwnj;</a></h4><p class="s9" style="padding-top: 8pt;padding-left: 10pt;text-indent: 0pt;text-align: left;"><a name="bookmark42">2.4.3.1. Stable Diffusion </a><span class="s18">的背景介绍</span></p><p style="padding-top: 7pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;"><span class="s9">2015 </span><a href="part18.htm#bookmark93" class="s6">年，一篇研究论文将统计物理学中的扩散模型引入到机器学习领域，为生成高质量图像样本提供了新思路</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">13</span>。它通过正向和逆向的扩散过程生成图像，其中正向过程逐步破坏数据分布的结构，逆向过程恢复图像的细节和结构。这也正是最近大火的基于扩散模型的方法 <span class="s9">(</span>如<a href="part18.htm#bookmark94" class="s5">Stale Diffusion</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">14</span>和<a href="part18.htm#bookmark95" class="s5">ControlNet</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">15</span><span class="s9">) </span>所使用的基本思想。</p><p style="padding-left: 38pt;text-indent: 0pt;text-align: left;">初始阶段生成的图像质量较差，而在五年后，另一篇由加州伯克</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-top: 4pt;padding-left: 10pt;text-indent: 18pt;line-height: 241%;text-align: left;"><a name="bookmark92">12</a><span class="s21"> Rombach R., et al. High-resolution image synthesis with latent diffusion models[C]. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684-10695, 2022.</span></p><p class="s23" style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark93">13 </a><span class="s21">Sohl-Dickstein J., et al. Deep unsupervised learning using nonequilibrium thermodynamics[C].</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s21" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">International conference on machine learning. PMLR, pages 2256-2265, 2015</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 10pt;text-indent: 18pt;line-height: 241%;text-align: left;"><a name="bookmark94">14 </a><span class="s21">Ramesh A., et al. Hierarchical text-conditional image generation with clip latents[J]. arXiv preprint arXiv:2204.06125, 2022.</span></p><p class="s20" style="padding-left: 10pt;text-indent: 18pt;line-height: 241%;text-align: left;"><a name="bookmark95">15</a><span class="s21">Rombach R., et al. High-resolution image synthesis with latent diffusion models[C]. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684-10695, 2022</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 10pt;text-indent: 0pt;line-height: 139%;text-align: justify;">利分校发表的开创性的研究论文提出了去噪扩散概率模型 <a href="part18.htm#bookmark96" class="s5">(DDPMs)</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">16</span>。<span class="s9">DDPM </span><a href="part18.htm#bookmark97" class="s6">是一种扩散生成模型，由两个参数化的马尔可夫链组成，它利用变分推断</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">17</span>在有限时间内生成与原始数据分布一致的样本。前向链逐渐将高斯噪声引入数据，使其分布趋近于标准高斯分布，而逆向链则通过参数化的高斯转换核逐步恢复原始数据分布。扩散模型在图像合成、计算机视觉、自然语言处理等领域展现出优异性能。</p><p style="padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: left;">尽管扩散模型已经在图像数据上取得了最先进的合成结果，扩散模型高昂的计算成本仍让众多研究人员望而却步。为了解决这个问题， 潜在扩散模型 <a href="part18.htm#bookmark98" class="s5">(Latent Diffusion Model, LDM)</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">18</span><span class="s9"> </span>的开发者 <span class="s9">CompVis </span>和 <span class="s9">Runway ML </span>将扩散模型应用于强大的预训练自编码器的潜在空间中，兼顾计算复杂度和图像细节的平衡。<span class="s9">LDM </span>在图像修复、条件图像合成等任务中取得了新的最优结果，并在多模态训练中拥有很好的表现。后续 <span class="s9">Stability AI </span>也一同联合开源了<span class="s9">LDM </span>的预训练模型，称为稳定扩散 <a href="part18.htm#bookmark99" class="s5">(Stable Diffusion)</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">19</span>。为了实现稳定扩散，研究人员使用了低秩矩阵来估计参数更新，从而在保持高质量图像生成的同时大幅减少了参数的数量。这种参数高效性能够在相对较小的计算资源下生成高质量的图像样本，该特性使得稳定扩散方法迅速席卷图像生成领域。</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 10pt;text-indent: 18pt;line-height: 241%;text-align: left;"><a name="bookmark96">16      </a><span class="s21">Ho J., et al. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.</span></p><p class="s20" style="padding-left: 10pt;text-indent: 18pt;line-height: 241%;text-align: left;"><a name="bookmark97">17</a><span class="s21"> Blei D. M., et al. Variational inference: A review for statisticians[J]. Journal of the American statistical Association, 2017, 112(518): 859-877.</span></p><p class="s23" style="padding-left: 10pt;text-indent: 18pt;line-height: 242%;text-align: left;"><a name="bookmark98">18 </a><span class="s21">Ramesh A., et al. Hierarchical text-conditional image generation with clip latents[J]. arXiv preprint arXiv:2204.06125, 2022.</span></p><p class="s23" style="padding-left: 28pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a name="bookmark99">19 </a><span class="s21">https://stability.ai/blog/stable-diffusion-public-release</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s9" style="padding-top: 3pt;padding-left: 10pt;text-indent: 0pt;text-align: left;"><a name="bookmark43">2.4.3.2 Stable Diffusion </a><span class="s18">的基本原理</span></p><p style="padding-top: 7pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">（<span class="s9">1</span>）去噪扩散概率模型（<span class="s9">DDPM</span>）是一种基于扩散模型的生成模型，用于对数据进行建模和生成。扩散模型的基本思想是通过一个前向的迭代扩散过程逐渐将高斯噪声引入数据，使其分布逐渐趋近于标准高斯分布。然后，通过学习一个逆向的扩散过程，将模糊的图像恢复到清晰的状态，从而重建数据中的结构。</p><p style="padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">为了训练<span class="s9">DDPM</span>，研究团队采用了去噪评分匹配框架。这种框架通过前向扩散过程将图像转化为噪声来定义图像的分布。通过训练去噪函数，使其最小化去噪评分匹配损失，<span class="s9">DDPM </span>可以从随机噪声中生成高质量的样本。</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 65pt;text-indent: 0pt;text-align: left;"><span><img width="403" height="240" alt="image" src="Image_036.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-top: 4pt;padding-left: 6pt;text-indent: 0pt;text-align: center;">图 <span class="s16">2-3 DDPM </span>生成结果图<span class="s16">(</span>来源：<a href="part18.htm#bookmark100" class="s43">DDPM</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; vertical-align: 4pt;">20</span><span class="s16">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s9" style="padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">DDPM <span class="p">在图像合成任务中表现出色，并在计算机视觉、自然语言处理、波形信号处理、多模态建模、分子图建模和时间序列建模等领域展示了优异的性能。它在无条件图像合成方面已被证明胜过生成对</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-left: 10pt;text-indent: 18pt;line-height: 241%;text-align: left;"><a name="bookmark100">20</a><span class="s21"> Ho J., et al. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.</span></p><p style="padding-top: 8pt;padding-left: 10pt;text-indent: 0pt;line-height: 139%;text-align: justify;">抗网络 <a href="part18.htm#bookmark101" class="s5">(GAN) </a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">21</span>，尽管 <span class="s9">GAN </span>在图像质量方面表现出色，但这些模型往往捕捉到的多样性较少，难以训练并容易出现模式崩溃问题。在图 <span class="s9">3-3 </span>中，左图是 <span class="s9">DDPM </span>在 <span class="s9">CelebA-HQ </span>数据集上通过去噪评分匹配训练后生成的高分辨率人脸图像，右图展示了 <span class="s9">DDPM </span>在<span class="s9">CIFAR10 </span>上仅依靠纯噪声生成的图片，即无条件合成的图片。</p><p style="padding-left: 10pt;text-indent: 27pt;line-height: 140%;text-align: justify;">（<span class="s9">2</span>）<span class="s9">DPM </span>包括前向过程（<span class="s9">forward process</span>）和反向过程（<span class="s9">reverse process</span>），其中前向过程又称为扩散过程（<span class="s9">diffusion process</span>）。</p><p style="text-indent: 0pt;text-align: left;"><span><img width="75" height="15" alt="image" src="Image_037.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="89" height="15" alt="image" src="Image_038.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="11" alt="image" src="Image_039.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="27" height="10" alt="image" src="Image_040.png"/></span></p><p style="padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">扩散过程 <span class="s9">(diffusion process)</span>，指对数据逐渐增加高斯噪音直至数据变成随机噪音的过程。对于原始数据 ，扩散过程的每一步都生成一个带噪音的数据 ，通过共计 步对上一步数据 逐渐增加高斯噪音。</p><p style="text-indent: 0pt;text-align: left;"><span><img width="67" height="15" alt="image" src="Image_041.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="10" alt="image" src="Image_042.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="84" height="16" alt="image" src="Image_043.png"/></span></p><p style="padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">与将数据噪声化的前向过程相反，反向过程<span class="s9">(reverse process)</span>是一个去噪过程，当反向过程每一步的真实分布 已知，那么从第 步的随机噪声图像 开始逐渐去噪便能生成真实图像。反向过程可定义为一个由一系列用神经网络参数化的高斯分布组成的马尔可夫链。</p><p style="padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: left;">模型设计方面，<span class="s9">DDPM </span>采用的是一个基于残差模块 <span class="s9">(residual block) </span>和注意力模块 <span class="s9">(attention block) </span>的 <span class="s9">U-Net </span>模型。<span class="s9">U-Net </span>属于编码器<span class="s9">-</span>解码器 <span class="s9">(encoder-decoder) </span>架构，其中编码器分成不同的 <span class="s9">stages</span>，每个<span class="s9">stage </span>都包含下采样模块来降低特征的空间大小。与编码器相反，解码器将压缩的特征逐渐恢复。<span class="s9">U-Net </span>在解码器模块中还引入了跳连接 <span class="s9">(skip connection)</span>，合并了编码器中间得到的同维度特征，这有利于网络的优化。</p><p class="s9" style="padding-left: 10pt;text-indent: 0pt;text-align: left;"><a name="bookmark44">2.4.3.3 </a><span class="s18">稳定扩散原理</span></p><p style="padding-top: 7pt;padding-left: 38pt;text-indent: 0pt;text-align: left;">稳定扩散（<span class="s9">Stable Diffusion</span>）是一种图像生成框架，通过在潜在</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark101">21</a><span class="s21"> Dhariwal P., et al. Diffusion models beat gans on image synthesis[J]. Advances in neural information</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s21" style="padding-left: 10pt;text-indent: 0pt;text-align: left;">processing systems, 2021, 34: 8780-8794.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="9" height="10" alt="image" src="Image_044.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="14" height="11" alt="image" src="Image_045.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="14" height="11" alt="image" src="Image_046.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="10" alt="image" src="Image_047.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="14" height="10" alt="image" src="Image_048.png"/></span></p><p style="padding-top: 3pt;padding-left: 10pt;text-indent: 0pt;line-height: 139%;text-align: justify;">空间上训练扩展的潜在扩散模型（<span class="s9">Latent Diffusion Model</span>，<span class="s9">LDM</span>）来生成高质量的图像样本。在稳定扩散的第一阶段，引入一个自编码器 <span class="s9">(AutoEncoder) </span>来学习特征的潜在表示，接着用编码器 对原始图像进行压缩编码，得到图像的低维表征 ，然后将 在潜在表示空间 <span class="s9">(latent space) </span>中进行扩散操作，最后将反向过程后的低维表征通过解码器 恢复到原始图像空间。由于 是经过压缩的潜在特征，其尺寸远小于原始图像，可显著降低扩散模型的计算成本。第二阶段的扩散过程与标准扩散模型类似，通过逐步引入噪声模糊潜在表示，然后逐步恢复图像的细节和结构。但稳定扩散中引入了条件机制 <span class="s9">(Conditioning Mechanism)</span>，将交叉注意力 <span class="s9">(cross-attention) </span>作为通用条件实现多模态训练。通过这种扩散过程，模型能够生成多样化且与条件信号相关的高质量图像样本。</p><p style="padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">实验结果表明（图 <span class="s9">3-4</span>），相较于在像素空间上进行扩散建模（<span class="s9">Pixel Baseline #1</span>），将扩散建模应用于潜在空间（<span class="s9">LDM #1,2,3</span>）能够在降低复杂度和保留图像细节方面取得显著优势。此外，条件机制的设计令稳定扩散能够更好地控制生成图像的多样性和真实性，它可以根据不同的条件生成多种语义合理图像。</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s21" style="padding-left: 37pt;text-indent: 0pt;text-align: left;">Input           GT        Pixel Baseline #1         LDM #1               LDM #2               LDM #3</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;text-align: left;"><span><img width="402" height="202" alt="image" src="Image_049.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-left: 53pt;text-indent: 0pt;text-align: center;">图 <span class="s16">2-4 Stable Diffusion </span>的生成结果图<span class="s16">(</span>来源：<a href="part18.htm#bookmark102" class="s43">Stable Diffusion</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; vertical-align: 4pt;">22</span><span class="s16">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-top: 4pt;padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark102">22 </a><span class="s21">Rombach R., et al. High-resolution image synthesis with latent diffusion models[C]. In Proceedings of the</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part17.htm">&lt; 上一个</a><span> | </span><a href="../AI%2BArt.html">内容</a><span> | </span><a href="part19.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
