<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>2.4.1 判别式技术发展综述</title><link href="navigation.css" rel="stylesheet" type="text/css"/><link href="document.css" rel="stylesheet" type="text/css"/></head><body><p class="top_nav"><a href="part15.htm">&lt; 上一个</a><span> | </span><a href="../AI%2BArt.html">内容</a><span> | </span><a href="part17.htm">下一个 &gt;</a></p><h4 style="padding-left: 10pt;text-indent: 0pt;text-align: left;"><a name="bookmark39">2.4.1 </a><span class="s17">判别式技术发展综述</span><a name="bookmark81">&zwnj;</a></h4><p style="padding-top: 7pt;padding-left: 10pt;text-indent: 27pt;line-height: 139%;text-align: justify;">判别式技术在<span class="s9">AIGC </span>领域中具有重要的作用，可以实现分类和预测、特征提取和表示学习、异常检测和异常行为识别，以及决策支持和优化等多种任务。它为我们理解和应用数据中的模式和信息提供了有效的工具和方法。判别式技术可以基于已有数据对样本进行分类或预测。判别式模型关注数据与标签之间的关系，通过学习特征和建立决策边界来进行分类。早期的判别式模型包括支持向量机、逻辑回归和深度神经网络。例如， <span class="s9">Alex Krizhevsky </span>等人在 <span class="s9">2012 </span>年提出的</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 28pt;text-indent: 0pt;text-align: left;"><a name="bookmark82">4 </a><span class="s21">Stable AI https://stability.ai/</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s23" style="padding-left: 10pt;text-indent: 18pt;line-height: 242%;text-align: left;"><a name="bookmark83">5 </a><span class="s21">Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets[J]. Advances in neural information processing systems, 2014, 27.</span></p><p style="padding-top: 8pt;padding-left: 10pt;text-indent: 0pt;line-height: 139%;text-align: left;"><span class="s9">AlexNet </span><a href="part16.htm#bookmark84" class="s6">模型</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">6</span>，通过深度卷积神经网络实现了在<span class="s9">ImageNet </span>图像分类竞赛中的突破。后来，<span class="s9">Vaswani </span>等人在 <span class="s9">2017 </span>年提出了<a href="part16.htm#bookmark85" class="s5">Transformers</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">7</span>，并在自然语言处理和计算机视觉领域取得了巨大的成功。 <span class="s9">Transformers </span>基于自注意力机制，通过对输入序列中不同位置的关系进行建模，实现了更好的特征表示和序列建模能力。现在，<span class="s9">Transformer</span>模型已成为视觉以及自然语言处理领域的重要基石，如 <span class="s9">Google </span>的 <a href="part16.htm#bookmark86" class="s5">BERT</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">8</span>就是基于这一模型。<a href="part16.htm#bookmark87" class="s5">Vision Transformer (ViT)</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; vertical-align: 4pt;">9</span>将自注意力机制</p><p style="padding-left: 10pt;text-indent: 0pt;line-height: 22pt;text-align: left;">引入计算机视觉领<span class="s26">域。它通</span>过将图像划分为一系列的图像块，并利用</p><p class="s9" style="padding-top: 3pt;padding-left: 10pt;text-indent: 0pt;line-height: 140%;text-align: left;">Transformer <span class="p">模型进行特征提取和分类，取得了与传统卷积神经网络相媲美的性能。</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-top: 4pt;padding-left: 10pt;text-indent: 18pt;line-height: 241%;text-align: left;"><a name="bookmark84">6</a><span class="s21"> Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105).</span></p><p class="s23" style="padding-left: 10pt;text-indent: 18pt;line-height: 242%;text-align: left;"><a name="bookmark85">7 </a><span class="s21">Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30</span></p><p class="s23" style="padding-left: 10pt;text-indent: 18pt;line-height: 241%;text-align: left;"><a name="bookmark86">8      </a><span class="s21">Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.</span></p><p class="s20" style="padding-left: 10pt;text-indent: 18pt;line-height: 241%;text-align: left;"><a name="bookmark87">9</a><span class="s21"> Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[J]. arXiv preprint arXiv:2010.11929, 2020.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 99pt;text-indent: 0pt;text-align: left;"><span><img width="314" height="271" alt="image" src="Image_035.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-top: 4pt;padding-left: 53pt;text-indent: 0pt;text-align: center;">图 <span class="s16">2-2 StyleGAN </span>生成结果图<span class="s16">(</span>来源：<a href="part17.htm#bookmark89" class="s43">StyleGAN</a><span style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; vertical-align: 4pt;">10</span><span class="s16">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="nav">&nbsp;&nbsp;</p><p class="nav">&nbsp;</p><p class="nav"><a href="part15.htm">&lt; 上一个</a><span> | </span><a href="../AI%2BArt.html">内容</a><span> | </span><a href="part17.htm">下一个 &gt;</a></p><p class="nav">&nbsp;&nbsp;</p></body></html>
